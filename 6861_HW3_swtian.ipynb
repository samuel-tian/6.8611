{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N655YeL2eEUC",
    "outputId": "b2bda4fa-888a-495b-9e42-d3700c563d7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hw3'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
    "rm -rf hw3\n",
    "git clone https://github.com/mit-6864/hw3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LH-OmiKv6Y_"
   },
   "source": [
    "# Part 1: Experiments with DistilBERT\n",
    "In this section, we will use our review dataset from HW1 and use a transformer based model, Distilbert, to see if it can perform better at the classification task. \n",
    "\n",
    "There isn't much implemention here, but we hope that you play around with the parameters and see how to use similar transformer models for custom datasets!\n",
    "\n",
    "## HuggingFace\n",
    "Hugging Face is a large open-source community for NLP research, and a great resource for pre-trained deep learning models. The Transformers library written in Python exposes a well-developed API to a plethora of deep learning architectures for SOTA performance in many NLP tasks. \n",
    "\n",
    "## Why DistilBERT \n",
    "The DistilBERT model was proposed in the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVbb7nCUwEQ5",
    "outputId": "86926111-bc90-49d1-a849-8b7e289f5919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r8dpX9AwhaL"
   },
   "source": [
    "# Processing Data\n",
    "\n",
    "In order to fine-tune a pretrained DistilBERT model, we must transform our dataset into the format that BERT can be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1x6xTytZwJnY"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"hw3\")\n",
    "\n",
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "np.random.seed(0)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lab_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nw7a6hstwLDl",
    "outputId": "84dc2490-d5b3-442d-be8d-893ce53ef648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
      "rating: 1 (good)\n",
      "\n",
      "Read 4000 total reviews.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"/content/hw3/reviews.csv\") as reader:\n",
    "  csvreader = csv.reader(reader)\n",
    "  next(csvreader)\n",
    "  for id, review, label in csvreader:\n",
    "    label = int(label)\n",
    "\n",
    "    # hacky class balancing\n",
    "    if label == 1:\n",
    "      if n_positive == 2000:\n",
    "        continue\n",
    "      n_positive += 1\n",
    "    if len(data) == 4000:\n",
    "      break\n",
    "\n",
    "    data.append((review, label))\n",
    "    \n",
    "    if n_disp > 5:\n",
    "      continue\n",
    "    n_disp += 1\n",
    "    print(\"review:\", review)\n",
    "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews, train_labels = list(reviews[:3000]), list(labels[:3000])\n",
    "val_reviews, val_labels = list(reviews[3000:3500]), list(labels[3000:3500])\n",
    "test_reviews, test_labels = list(reviews[3500:]), list(labels[3500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBDwHw0owHsb"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. Luckily, this tokenizer is available with the HuggingFace library. We'll be using the 'cased' version here. \n",
    "\n",
    "For the tokenization, we need to perform the following steps: \n",
    "- Pad or truncate all reviews to a single constant length \n",
    "- Add special tokens to the start and end.  \n",
    "- Explicitely differentiate real tokens from padding tokens with the attention mask. \n",
    "\n",
    "## Tokens - What tokens do we need to append / prepend \n",
    "- [CLS] at the start of the sequence\n",
    "- [SEP] at the end of the sequence\n",
    "\n",
    "“The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.” [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "The \"attention mask\" tells the model which tokens should be attended to and which (the [PAD] tokens) should not ([see the documentation for more detail](https://huggingface.co/docs/transformers/glossary#attention-mask)).\n",
    "\n",
    "![A demonstration of tokenization in BERT](https://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qhxb7b7bwson"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cbxDqk_cws-4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "def tokenizer_function(input_data, labels):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  for sent in input_data:\n",
    "    this_encoding = tokenizer.encode_plus(sent, truncation=True, pad_to_max_length = True,max_length = 512,return_attention_mask = True,return_tensors = 'pt')\n",
    "    input_ids.append(this_encoding['input_ids'])\n",
    "    attention_masks.append( this_encoding['attention_mask'])\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "  labels = torch.tensor(labels)\n",
    "  tokenized_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "  return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgR5B8WnwtOm",
    "outputId": "d2d64283-4181-4a47-92d3-af66d256c162"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenizer_function(reviews[:3000], labels[:3000])\n",
    "val_dataset = tokenizer_function(reviews[3000:3500], labels[3000:3500])\n",
    "test_dataset = tokenizer_function(reviews[3500:], labels[3500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrYhywlsw0SF"
   },
   "source": [
    "Create a DataLoader for your training and test datasets so you can iterate over batches of data. (Hint: Make the batch size small if you don't want to overwhelm your colab). Feel free to experiment with batch_size and learning rate. You can also try different optimizers and see which performs better!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8CgfBenw6k-",
    "outputId": "fd0c7edd-0109-4c78-e609-1bc3ae9fb67c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2, output_attentions = False, output_hidden_states=False)\n",
    "model.to(device)\n",
    "\n",
    "batch_size_ = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_, shuffle=True) #Feel free to experiment with batch sizes\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ESekx-ziw8nc"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NvGxGMOKw9KW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3nVtJ4VxBUQ"
   },
   "source": [
    "## Analyze the parameters in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tD2YRQOxEVz",
    "outputId": "a7abb10e-2798-4117-801f-e3ea2eeff4ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 104 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "distilbert.embeddings.word_embeddings.weight            (28996, 768)\n",
      "distilbert.embeddings.position_embeddings.weight          (512, 768)\n",
      "distilbert.embeddings.LayerNorm.weight                        (768,)\n",
      "distilbert.embeddings.LayerNorm.bias                          (768,)\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight     (768, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight     (768, 768)\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight     (768, 768)\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias           (768,)\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight   (768, 768)\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias         (768,)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight           (768,)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias             (768,)\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight           (3072, 768)\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias                 (3072,)\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight           (768, 3072)\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias                  (768,)\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight       (768,)\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias         (768,)\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight     (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pre_classifier.weight                                     (768, 768)\n",
      "pre_classifier.bias                                           (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tnkX1QgbxKFl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9lOqU7JxNBL"
   },
   "source": [
    "Now it is time to train. Notice how it is working, it might potentially help with your project as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljTr7ARRxPiO",
    "outputId": "b0bc2625-2401-4bdf-cd50-c79cda37c4d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "  Average training loss: 0.45\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [02:25<04:50, 145.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.84\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:00:09\n",
      "tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "  Average training loss: 0.24\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [04:50<02:25, 145.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.32\n",
      "  Validation took: 0:00:09\n",
      "tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "  Average training loss: 0.13\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [07:15<00:00, 145.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.44\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:07:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "for epoch in tqdm(range(3)):\n",
    "  total_train_loss = 0\n",
    "  model.train()\n",
    "  for step, batch in enumerate(train_loader):\n",
    "    \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    model.zero_grad()  \n",
    "    outputs = model(b_input_ids, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "    loss = outputs.loss\n",
    "    total_train_loss += loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if(step %10 == 0):\n",
    "      print(loss)\n",
    "\n",
    "  avg_train_loss = total_train_loss / len(train_loader)   \n",
    "  print(\"\")\n",
    "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "      \n",
    "  print(\"\")\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  # Put the model in evaluation mode-\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "  nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "  for batch in val_loader:\n",
    "      #\n",
    "      # `batch` contains three pytorch tensors:\n",
    "      #   [0]: input ids \n",
    "      #   [1]: attention masks\n",
    "      #   [2]: labels \n",
    "      b_input_ids = batch[0].to(device)\n",
    "      b_input_mask = batch[1].to(device)\n",
    "      b_labels = batch[2].to(device)\n",
    "\n",
    "      with torch.no_grad():        \n",
    "\n",
    "          outputs = model(b_input_ids, \n",
    "                                  attention_mask=b_input_mask,\n",
    "                                  labels=b_labels)\n",
    "          \n",
    "      # Accumulate the validation loss.\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "      total_eval_loss += loss.item()\n",
    "\n",
    "      # Move logits and labels to CPU\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "      # Calculate the accuracy for this batch of test sentences, and\n",
    "      # accumulate it over all batches.\n",
    "      total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "      \n",
    "\n",
    "  # Report the final accuracy for this validation run.\n",
    "  avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "  # Calculate the average loss over all of the batches.\n",
    "  avg_val_loss = total_eval_loss / len(val_loader)\n",
    "  \n",
    "  # Measure how long the validation run took.\n",
    "  validation_time = format_time(time.time() - t0)\n",
    "  \n",
    "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "  print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:}\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkhEjd9WxSRy"
   },
   "source": [
    "Now we can check how our accuracy is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de6g2qv1xVnN",
    "outputId": "b122bef4-5135-45ec-81d1-4dc1943286b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 500 test sentences...\n",
      "Final Accuracy: 0.866\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(reviews[3500:])))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_loader:\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids = batch[0].to(device)\n",
    "  b_input_mask = batch[1].to(device)\n",
    "  b_labels = batch[2].to(device)\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids,\n",
    "                      attention_mask=b_input_mask)\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.extend(logits)\n",
    "  true_labels.extend(label_ids)\n",
    "print('Final Accuracy: {0}'.format(flat_accuracy(np.asarray(predictions), np.asarray(true_labels))))\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaoYiysseNIH"
   },
   "source": [
    "## Part 2: Hidden Markov Models\n",
    "\n",
    "In Part 2 of this homework, you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. This uses the same dataset and lab_util as in HW 2. \n",
    "\n",
    "As in previous homeworks, we'll start by loading up a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "g5R8vijdeKgl"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/hw3\")\n",
    "\n",
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "np.random.seed(0)\n",
    "\n",
    "import lab_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUn-q_pIeuAV",
    "outputId": "04639489-0ccc-4fa3-8a2e-5440001aec9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
      "rating: 1 (good)\n",
      "\n",
      "Read 4000 total reviews.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"/content/hw3/reviews.csv\") as reader:\n",
    "  csvreader = csv.reader(reader)\n",
    "  next(csvreader)\n",
    "  for id, review, label in csvreader:\n",
    "    label = int(label)\n",
    "\n",
    "    # hacky class balancing\n",
    "    if label == 1:\n",
    "      if n_positive == 2000:\n",
    "        continue\n",
    "      n_positive += 1\n",
    "    if len(data) == 4000:\n",
    "      break\n",
    "\n",
    "    data.append((review, label))\n",
    "    \n",
    "    if n_disp > 5:\n",
    "      continue\n",
    "    n_disp += 1\n",
    "    print(\"review:\", review)\n",
    "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2qlqRHoe3y-"
   },
   "source": [
    "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
    "\n",
    "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(exp(a) + exp(b)) = logaddexp(a, b)`). You should use the first hint whenever you want to multiply/divide numbers by adding them in log space instead, and you should use the second hint whenever you want to add numbers that are already in or could be converted to log space. In general, we recommend either `np.logaddexp` or `scipy.special.logsumexp` as safe ways to compute the necessary quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "_wVf4QVIfBdc"
   },
   "outputs": [],
   "source": [
    "# hmm model\n",
    "from scipy.special import softmax\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class HMM(object):\n",
    "    def __init__(self, num_states, num_words):\n",
    "        self.num_states = num_states\n",
    "        self.num_words = num_words\n",
    "\n",
    "        self.states = range(num_states)\n",
    "        self.symbols = range(num_words)\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the matrix A with random transition probabilities p(j|i)\n",
    "        from a uniform distribution.\n",
    "        A should be a matrix of size `num_states x num_states` with rows that\n",
    "        sum to 1.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        self.A = np.random.rand(num_states, num_states)\n",
    "        self.A = self.A / np.sum(self.A, axis=-1, keepdims=True)\n",
    "        # Your code ends\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the matrix B with random emission probabilities p(o|i) from a \n",
    "        uniform distribution. \n",
    "        B should be a matrix of size `num_states x num_words` with rows that sum \n",
    "        to 1.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        self.B = np.random.rand(num_states, num_words)\n",
    "        self.B = self.B / np.sum(self.B, axis=-1, keepdims=True)\n",
    "        # Your code ends\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the vector pi with a random starting distribution chosen from \n",
    "        a uniform distribution. \n",
    "        pi should be a vector of size `num_states` with entries that sum to 1.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        self.pi = np.random.rand(num_states)\n",
    "        self.pi = self.pi / np.sum(self.pi, axis=-1, keepdims=True)\n",
    "        # Your code ends\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"randomly sample the HMM to generate a sequence.\n",
    "        \"\"\"\n",
    "        # we'll give you this one\n",
    "\n",
    "        sequence = []\n",
    "        # initialize the first state\n",
    "        state = np.random.choice(self.states, p=self.pi)\n",
    "        for i in range(n):\n",
    "            # get the emission probs for this state\n",
    "            b = self.B[state, :]\n",
    "            # emit a word\n",
    "            word = np.random.choice(self.symbols, p=b)\n",
    "            sequence.append(word)\n",
    "            # get the transition probs for this state\n",
    "            a = self.A[state, :]\n",
    "            # update the state\n",
    "            state = np.random.choice(self.states, p=a)\n",
    "        return sequence\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Runs the forward algorithm. This function should return a \n",
    "        `len(obs) x  num_states` matrix where the (t, i)th entry contains \n",
    "        log p(obs[:t], hidden_state_t = i)\n",
    "        \"\"\"\n",
    "\n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "\n",
    "        alpha = np.zeros((len(obs), self.num_states))\n",
    "        alpha[0] = np.log(self.pi) + log_B.T[obs[0]]\n",
    "\n",
    "        for t in range(1, len(obs)):\n",
    "            sum = logsumexp(alpha[t-1] + log_A.T, axis=1)\n",
    "            alpha[t] = sum + log_B.T[obs[t]]\n",
    "\n",
    "        # Your code ends\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def backward(self, obs):\n",
    "        \"\"\"\n",
    "        Run the backward algorithm. This function should return a\n",
    "        `len(obs) x num_states` matrix where the (t, i)th entry contains\n",
    "        log p(obs[t+1:] | hidden_state_t = i)\n",
    "        \"\"\"\n",
    "\n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "        beta = np.zeros((len(obs), self.num_states))\n",
    "\n",
    "        # Your code here\n",
    "        for t in range(len(obs)-2, -1, -1):\n",
    "            beta[t] = logsumexp(beta[t+1] + log_A + log_B.T[obs[t+1]], axis=1)\n",
    "\n",
    "        # Your code ends\n",
    "\n",
    "        return beta\n",
    "        \n",
    "    def forward_backward(self, obs):\n",
    "        \"\"\"\n",
    "        Compute forward-backward scores\n",
    "\n",
    "        logprob is the total log-probability of the sequence obs (marginalizing\n",
    "        over hidden states).\n",
    "\n",
    "        gamma is a matrix of size `len(obs) x num_states`. It contains the\n",
    "        marginal probability of being in state i at time t\n",
    "\n",
    "        xi is a tensor of size `len(obs) - 1 x num_states x num_states`. It contains\n",
    "        the marginal probability of transitioning from i to j at t.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code here\n",
    "\n",
    "        log_A = np.log(self.A)\n",
    "        log_B = np.log(self.B)\n",
    "        alpha = self.forward(obs)\n",
    "        beta = self.backward(obs)\n",
    "\n",
    "        logprob = logsumexp(alpha[-1])\n",
    "        xi = np.zeros((len(obs)-1, self.num_states, self.num_states))\n",
    "        gamma = np.zeros((len(obs), self.num_states))\n",
    "\n",
    "        for t in range(len(obs)-1):\n",
    "            xi[t] = alpha[t:t+1].T + beta[t+1:t+2] + log_A + log_B[:, obs[t+1]] - logprob\n",
    "        xi = np.exp(xi)\n",
    "        # Your code ends\n",
    "\n",
    "        for t in range(len(obs)):\n",
    "            gamma[t] = alpha[t] + beta[t] - logprob\n",
    "        gamma = np.exp(gamma)\n",
    "\n",
    "        return logprob, xi, gamma\n",
    "\n",
    "        \"\"\"\n",
    "        SANITY CHECK\n",
    "\n",
    "        The most straightforward way of implementing the forward, backward, and \n",
    "        forward_backward methods would be to iterate through all the values and \n",
    "        use the formulas in the slides to calculate the corresponding values.\n",
    "\n",
    "        However, this may not be fast enough. If your model is taking too long\n",
    "        to train, consider how you may speed up your code by reducing the number\n",
    "        of for loops involved. How can you reformulate your code using matrix\n",
    "        operations?\n",
    "\n",
    "        Hint: we were able to implement each of the forward, backward, and\n",
    "        forward_backward operations using only one for loop.\n",
    "        \"\"\"\n",
    "\n",
    "    def learn_unsupervised(self, corpus, num_iters, print_every=10):\n",
    "        \"\"\"Run the Baum Welch EM algorithm\n",
    "        \n",
    "        corpus: the data to learn from\n",
    "        num_iters: the number of iterations to run the algorithm\n",
    "        print_every: how often to print the log-likelihood while the model is\n",
    "        updating its parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        for i_iter in tqdm(range(num_iters)):\n",
    "            \"\"\"\n",
    "            expected_si: a vector of size (num_states,) where the i-th entry is\n",
    "            the expected number of times a sentence is transitioning from state \n",
    "            i to some other state. Be careful about which states this includes!\n",
    "\n",
    "            expected_sij: an array of size (num_states, num_states) where the\n",
    "            (i,j)-th entry represents the expected number of state transitions\n",
    "            between state i and state j.\n",
    "\n",
    "            expected_sjwk: an array of size (num_states, num_words) where the \n",
    "            (j,k)-th entry represents the expected number of times the word w_k \n",
    "            appears when at state j.\n",
    "\n",
    "            expected_q1: a vector of size (num_states,) where the i-th entry is \n",
    "            the expected number of times state i is the first state.\n",
    "\n",
    "            total_logprob: The log of the probability of the corpus being\n",
    "            generated with the current parameters of the HMM.\n",
    "            \"\"\"\n",
    "            expected_si = np.zeros((self.num_states)) # your code here\n",
    "            expected_si_ = np.zeros((self.num_states))\n",
    "            expected_sij = np.zeros((self.num_states, self.num_states)) # your code here\n",
    "            expected_sjwk = np.zeros((self.num_states, self.num_words)) # your code here\n",
    "            expected_q1 = np.zeros((self.num_states)) # your code here\n",
    "            total_logprob = 0\n",
    "            \n",
    "            for review in corpus:\n",
    "                logprob, xi, gamma = self.forward_backward(review)\n",
    "                # Your code here \n",
    "\n",
    "                expected_si += (np.sum(gamma, axis=0) - gamma[-1])\n",
    "                expected_si_ += np.sum(gamma, axis=0)\n",
    "                expected_sij += np.sum(xi, axis=0)\n",
    "                expected_q1 += gamma[0]\n",
    "                total_logprob += logprob\n",
    "\n",
    "                for t in range(len(review)):\n",
    "                    expected_sjwk[:, review[t]] += gamma[t]\n",
    "\n",
    "\n",
    "                # Your code ends\n",
    "            if i_iter % print_every == 0:\n",
    "                print(\"log-likelihood\", total_logprob)\n",
    "\n",
    "            \"\"\"\n",
    "            The following variables should be the new values of self.A, self.B,\n",
    "            and self.pi after the values are updated.\n",
    "            \"\"\"\n",
    "            A_new = (expected_sij.T / expected_si).T # your code here\n",
    "            B_new = expected_sjwk / np.expand_dims(expected_si_, axis=1) # your code here\n",
    "            pi_new = expected_q1 / len(corpus) # your code here\n",
    "\n",
    "            # print(np.sum(A_new, axis=-1))\n",
    "            # print(np.sum(B_new, axis=-1))\n",
    "            # print(np.sum(pi_new, axis=-1))\n",
    "\n",
    "            # A_new = A_new / np.sum(A_new, axis=-1, keepdims=True)\n",
    "            # B_new = B_new / np.sum(B_new, axis=-1, keepdims=True)\n",
    "            # pi_new = pi_new / np.sum(pi_new, axis=-1, keepdims=True)\n",
    "\n",
    "            self.A = A_new\n",
    "            self.B = B_new\n",
    "            self.pi = pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yFF2rKuuh6U"
   },
   "source": [
    "## Test Cases\n",
    "\n",
    "The following are test cases that are meant to help you debug your code. The code involves six test suites - an initialization test, a forward test, a backward test, a forward_backward test, a baum_welch_update test, and a final end_to_end test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y3gRZ_r_vLav"
   },
   "outputs": [],
   "source": [
    "def init_test():\n",
    "\n",
    "    num_states = np.random.randint(100)\n",
    "    num_words = np.random.randint(100)\n",
    "    model = HMM(num_states, num_words)\n",
    "\n",
    "    assert model.A.shape == (num_states, num_states)\n",
    "    assert model.B.shape == (num_states, num_words)\n",
    "    assert model.pi.shape == (num_states, )\n",
    "\n",
    "    assert np.linalg.norm(np.sum(model.A, axis=1) - np.ones(num_states)) < 1e-10\n",
    "    assert np.linalg.norm(np.sum(model.B, axis=1) - np.ones(num_states)) < 1e-10\n",
    "    assert np.linalg.norm(np.sum(model.pi) - 1) < 1e-10\n",
    "\n",
    "def forward_test():\n",
    "    model = HMM(2, 10)\n",
    "    model.A = np.array([[0.79034887, 0.20965113],\n",
    "                        [0.66824331, 0.33175669]])\n",
    "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
    "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
    "    model.pi = np.array([0.77480039, 0.22519961])\n",
    "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
    "    alpha = model.forward(obs)\n",
    "\n",
    "    print(\"The result of the forward function should be\", np.array([[-2.96913, -3.43382],\n",
    "                                                                    [ -4.66005, -9.19418],\n",
    "                                                                    [ -7.35001, -7.89695],\n",
    "                                                                    [ -9.65069, -9.95363],\n",
    "                                                                    [-11.25815, -14.27392],\n",
    "                                                                    [-18.14079, -14.4781 ],\n",
    "                                                                    [-16.89275, -18.62696],\n",
    "                                                                    [-19.45549, -20.17289],\n",
    "                                                                    [-21.53772, -23.283  ],\n",
    "                                                                    [-23.4927, -26.69119],\n",
    "                                                                    [-25.84891, -26.73817],\n",
    "                                                                    [-28.12237, -29.92402]]))\n",
    "    print(\"Your value of alpha is:\", np.round(alpha, 5))\n",
    "\n",
    "def backward_test():\n",
    "    model = HMM(2, 10)\n",
    "    model.A = np.array([[0.79034887, 0.20965113],\n",
    "                        [0.66824331, 0.33175669]])\n",
    "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
    "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
    "    model.pi = np.array([0.77480039, 0.22519961])\n",
    "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
    "    beta = model.backward(obs)\n",
    "\n",
    "    print(\"The result of the backward function should be\", np.array([[-25.42937, -25.58918], \n",
    "                                                                     [-23.32164, -23.19959],\n",
    "                                                                     [-21.11007, -21.02033],\n",
    "                                                                     [-18.82215, -18.94381],\n",
    "                                                                     [-16.78523, -16.33951],\n",
    "                                                                     [-13.42847, -13.51924],\n",
    "                                                                     [-11.24815, -11.19161],\n",
    "                                                                     [ -8.88679,  -8.96441],\n",
    "                                                                     [ -6.57374,  -6.70985],\n",
    "                                                                     [ -4.51873,  -4.47419],\n",
    "                                                                     [ -2.44529,  -2.51463],\n",
    "                                                                     [  0, 0]]))\n",
    "\n",
    "    print(\"Your value of beta is:\", np.round(beta, 5))\n",
    "\n",
    "\n",
    "def forward_backward_test():\n",
    "    model = HMM(2, 10)\n",
    "    model.A = np.array([[0.79034887, 0.20965113],\n",
    "                        [0.66824331, 0.33175669]])\n",
    "    model.B = np.array([[0.08511814, 0.06627238, 0.08487461, 0.15607959, 0.00124582, 0.12984083, 0.11164849, 0.11591902, 0.15232716, 0.09667395],\n",
    "                        [0.18425462, 0.14326559, 0.14026994, 0.0215989,  0.17687124, 0.04681278, 0.05857451, 0.17451212, 0.00473382, 0.04910648]])\n",
    "    model.pi = np.array([0.77480039, 0.22519961])\n",
    "    obs = [1, 8, 0, 0, 3, 4, 5, 2, 6, 3, 7, 9]\n",
    "    logprob, xi, gamma = model.forward_backward(obs)\n",
    "\n",
    "    print(\"The value of logprob should be:\", -27.9693)\n",
    "    print(\"Your value of logprob is:\", np.round(logprob, 5))\n",
    "\n",
    "    print(\"The value of xi should be:\", np.array([[[0.64523, 0.00601],\n",
    "                                                  [0.34278, 0.00598]],\n",
    "\n",
    "                                                 [[0.60684, 0.38117],\n",
    "                                                  [0.00551, 0.00648]],\n",
    "\n",
    "                                                 [[0.40595, 0.2064 ],\n",
    "                                                  [0.19863, 0.18902]],\n",
    "\n",
    "                                                 [[0.5718,  0.03278],\n",
    "                                                  [0.35711, 0.03831]],\n",
    "\n",
    "                                                 [[0.02625, 0.90266],\n",
    "                                                  [0.00109, 0.07   ]],\n",
    "\n",
    "                                                 [[0.02482, 0.00251],\n",
    "                                                  [0.81777, 0.15489]],\n",
    "\n",
    "                                                 [[0.59943, 0.24316],\n",
    "                                                  [0.08947, 0.06793]],\n",
    "\n",
    "                                                 [[0.6143,  0.07461],\n",
    "                                                  [0.25347, 0.05762]],\n",
    "\n",
    "                                                 [[0.8357,  0.03207],\n",
    "                                                  [0.12337, 0.00886]],\n",
    "\n",
    "                                                 [[0.69872, 0.26034],\n",
    "                                                  [0.02412, 0.01682]],\n",
    "\n",
    "                                                 [[0.63701, 0.08583],\n",
    "                                                  [0.22134, 0.05582]]]))\n",
    "    print(\"Your value of xi is:\", np.round(xi, 5))\n",
    "\n",
    "    print(\"The value of gamma should be:\", np.array([[0.65124, 0.34876],\n",
    "                                                    [0.98802, 0.01198],\n",
    "                                                    [0.61235, 0.38765],\n",
    "                                                    [0.60458, 0.39542],\n",
    "                                                    [0.92891, 0.07109],\n",
    "                                                    [0.02733, 0.97267],\n",
    "                                                    [0.8426,  0.1574 ],\n",
    "                                                    [0.68891, 0.31109],\n",
    "                                                    [0.86777, 0.13223],\n",
    "                                                    [0.95906, 0.04094],\n",
    "                                                    [0.72284, 0.27716],\n",
    "                                                    [0.85835, 0.14165]]))\n",
    "\n",
    "    print(\"Your value of gamma is:\", np.round(gamma, 5))\n",
    "\n",
    "def baum_welch_update_test():\n",
    "    model = HMM(4, 10)\n",
    "    \n",
    "    model.A = np.array([[0.05263151, 0.62161178, 0.06683182, 0.25892489],\n",
    "                        [0.26993274, 0.13114741, 0.32305468, 0.27586517],\n",
    "                        [0.2951958,  0.14576492, 0.22474111, 0.33429817],\n",
    "                        [0.29586018, 0.26065884, 0.1977772,  0.24570378]])\n",
    "    \n",
    "    model.B = np.array([[0.01800425, 0.09767131, 0.17824799, 0.12586453, 0.19514548, 0.05433139, 0.01995667, 0.12985343, 0.01884263, 0.16208232],\n",
    "                        [0.04512782, 0.09469685, 0.1426164,  0.13851362, 0.08717793, 0.17152532, 0.08746939, 0.04900339, 0.05315859, 0.13071069],\n",
    "                        [0.11055806, 0.10592473, 0.0051817,  0.07721441, 0.21761783, 0.20323146, 0.18881598, 0.00584989, 0.00682669, 0.07877924],\n",
    "                        [0.08711377, 0.16703645, 0.0706214,  0.05297571, 0.10486868, 0.16794587, 0.13562053, 0.15729142, 0.03345308, 0.02307309]])\n",
    "    \n",
    "    model.pi = np.array([0.21186864, 0.27156561, 0.37188523, 0.14468051])\n",
    "    \n",
    "    corpus = np.array([[7,3,2,5,0,3,2,9,4,2], [7,3,2,4,2,8,7,5,0,8], [7,3,2,3,1,7,3,8,6,7], [7,3,2,6,4,4,3,4,0,0]])\n",
    "\n",
    "    model.learn_unsupervised(corpus, 200)\n",
    "\n",
    "    print(\"hmm.A should be\\n\", np.array([[0, 1, 0, 0], \n",
    "                                     [0.14122, 0, 0.27099, 0.58779], \n",
    "                                     [0.20671, 0, 0, 0.79329], \n",
    "                                     [0, 0.90909, 0.09091, 0]]))\n",
    "    print(\"Your implementation has hmm.A to be\\n\", np.round(model.A, 5))\n",
    "\n",
    "    print(\"hmm.B should be\\n\", np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                                              [0.0625, 0, 0, 0.5, 0, 0.125, 0.125, 0, 0.125, 0.0625],\n",
    "                                              [0, 0.20671, 0, 0, 0.79329, 0, 0, 0, 0, 0],\n",
    "                                              [0.24667, 0, 0.57555, 0, 0.09556, 0, 0, 0, 0.08222, 0]]))\n",
    "    print(\"Your implementation has hmm.B to be\\n\", np.round(model.B, 5))\n",
    "\n",
    "    print(\"hmm.pi should be\\n\", np.array([1, 0, 0, 0]))\n",
    "\n",
    "    print(\"Your implementation has hmm.pi to be\\n\", np.round(model.pi, 5))\n",
    "\n",
    "def end_to_end_test():\n",
    "    # Test Case 1\n",
    "\n",
    "    corpus = np.array([[0,3,0,3,0,3,0,3,0,3,0,3], [0,2,0,2,0,2,0,2,0,2,0,2,0], [1,2,1,2,1,2,1,2,1,2,1,2],[1,3,1,3,1,3,1,3,1,3]])\n",
    "    hmm = HMM(num_states=2,num_words=4)\n",
    "    hmm.learn_unsupervised(corpus, 10)\n",
    "    print(\"After this test case, hmm.A should either be approximately,\",  np.array([[0, 1], [1, 0]]))\n",
    "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\n",
    "\n",
    "    print(\"After this test case, hmm.B should either be approximately,\", np.array([[0, 0, 0.5, 0.5], [0.5, 0.5, 0, 0]]), \" or it should be \", np.array([[0.5, 0.5, 0, 0], [0, 0, 0.5, 0.5]]))\n",
    "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))\n",
    "\n",
    "    # Test Case 2\n",
    "\n",
    "    corpus = np.array([[0,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], [2,2,2,2,2,2,2,2,2,2]])\n",
    "    hmm = HMM(num_states=3, num_words=3)\n",
    "    hmm.learn_unsupervised(corpus, 100)\n",
    "    print(\"After this test case, hmm.A should be the identity matrix\", np.eye(3))\n",
    "    print(\"This is your current value of hmm.A: \", np.round(hmm.A, 5))\n",
    "\n",
    "    print(\"After this test case, hmm.B should be some 3 by 3 permutation matrix\")\n",
    "    print(\"This is your current value of hmm.B: \", np.round(hmm.B, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmewPFV2MPlS"
   },
   "source": [
    "## Test\n",
    "\n",
    "To actually run the test cases, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdA8HhciUuXs",
    "outputId": "14c619a3-3f33-4617-a394-80c65861c7d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [00:00<00:01, 159.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -96.33989919755487\n",
      "log-likelihood -61.83737464303459\n",
      "log-likelihood -59.659076127854746\n",
      "log-likelihood -58.780061682331436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 63/200 [00:00<00:01, 131.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.056386229045756\n",
      "log-likelihood -58.047573998672505\n",
      "log-likelihood -58.04756024248874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 102/200 [00:00<00:00, 160.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.047560146661965\n",
      "log-likelihood -58.04756014303575\n",
      "log-likelihood -58.04756014156314\n",
      "log-likelihood -58.04756014077337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 153/200 [00:00<00:00, 160.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.0475601403471\n",
      "log-likelihood -58.047560140117014\n",
      "log-likelihood -58.04756013999281\n",
      "log-likelihood -58.04756013992578\n",
      "log-likelihood -58.04756013988958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 187/200 [00:01<00:00, 154.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.047560139870065\n",
      "log-likelihood -58.04756013985951\n",
      "log-likelihood -58.047560139853836\n",
      "log-likelihood -58.04756013985076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 200/200 [00:01<00:00, 154.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm.A should be\n",
      " [[0.      1.      0.      0.     ]\n",
      " [0.14122 0.      0.27099 0.58779]\n",
      " [0.20671 0.      0.      0.79329]\n",
      " [0.      0.90909 0.09091 0.     ]]\n",
      "Your implementation has hmm.A to be\n",
      " [[0.      1.      0.      0.     ]\n",
      " [0.14122 0.      0.27099 0.58779]\n",
      " [0.20671 0.      0.      0.79329]\n",
      " [0.      0.90909 0.09091 0.     ]]\n",
      "hmm.B should be\n",
      " [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.     ]\n",
      " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
      "  0.0625 ]\n",
      " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
      "  0.     ]\n",
      " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
      "  0.     ]]\n",
      "Your implementation has hmm.B to be\n",
      " [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.     ]\n",
      " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
      "  0.0625 ]\n",
      " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
      "  0.     ]\n",
      " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
      "  0.     ]]\n",
      "hmm.pi should be\n",
      " [1 0 0 0]\n",
      "Your implementation has hmm.pi to be\n",
      " [1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.seterr(divide = 'ignore') \n",
    "# init_test()\n",
    "# forward_backward_test()\n",
    "baum_welch_update_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VFtzZ9W9MVKu",
    "outputId": "5ccca2ee-930e-4e6c-b3e1-29ef3d9d987d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the forward function should be [[ -2.96913  -3.43382]\n",
      " [ -4.66005  -9.19418]\n",
      " [ -7.35001  -7.89695]\n",
      " [ -9.65069  -9.95363]\n",
      " [-11.25815 -14.27392]\n",
      " [-18.14079 -14.4781 ]\n",
      " [-16.89275 -18.62696]\n",
      " [-19.45549 -20.17289]\n",
      " [-21.53772 -23.283  ]\n",
      " [-23.4927  -26.69119]\n",
      " [-25.84891 -26.73817]\n",
      " [-28.12237 -29.92402]]\n",
      "Your value of alpha is: [[ -2.96913  -3.43382]\n",
      " [ -4.66005  -9.19418]\n",
      " [ -7.35001  -7.89695]\n",
      " [ -9.65069  -9.95363]\n",
      " [-11.25815 -14.27392]\n",
      " [-18.14079 -14.4781 ]\n",
      " [-16.89275 -18.62697]\n",
      " [-19.45549 -20.17289]\n",
      " [-21.53772 -23.283  ]\n",
      " [-23.4927  -26.69119]\n",
      " [-25.84891 -26.73817]\n",
      " [-28.12237 -29.92402]]\n",
      "The result of the backward function should be [[-25.42937 -25.58918]\n",
      " [-23.32164 -23.19959]\n",
      " [-21.11007 -21.02033]\n",
      " [-18.82215 -18.94381]\n",
      " [-16.78523 -16.33951]\n",
      " [-13.42847 -13.51924]\n",
      " [-11.24815 -11.19161]\n",
      " [ -8.88679  -8.96441]\n",
      " [ -6.57374  -6.70985]\n",
      " [ -4.51873  -4.47419]\n",
      " [ -2.44529  -2.51463]\n",
      " [  0.        0.     ]]\n",
      "Your value of beta is: [[-25.42937 -25.58918]\n",
      " [-23.32164 -23.19959]\n",
      " [-21.11007 -21.02033]\n",
      " [-18.82215 -18.94381]\n",
      " [-16.78523 -16.33951]\n",
      " [-13.42847 -13.51924]\n",
      " [-11.24815 -11.19161]\n",
      " [ -8.88679  -8.96441]\n",
      " [ -6.57374  -6.70985]\n",
      " [ -4.51873  -4.47419]\n",
      " [ -2.44529  -2.51463]\n",
      " [  0.        0.     ]]\n",
      "The value of logprob should be: -27.9693\n",
      "Your value of logprob is: -27.96963\n",
      "The value of xi should be: [[[0.64523 0.00601]\n",
      "  [0.34278 0.00598]]\n",
      "\n",
      " [[0.60684 0.38117]\n",
      "  [0.00551 0.00648]]\n",
      "\n",
      " [[0.40595 0.2064 ]\n",
      "  [0.19863 0.18902]]\n",
      "\n",
      " [[0.5718  0.03278]\n",
      "  [0.35711 0.03831]]\n",
      "\n",
      " [[0.02625 0.90266]\n",
      "  [0.00109 0.07   ]]\n",
      "\n",
      " [[0.02482 0.00251]\n",
      "  [0.81777 0.15489]]\n",
      "\n",
      " [[0.59943 0.24316]\n",
      "  [0.08947 0.06793]]\n",
      "\n",
      " [[0.6143  0.07461]\n",
      "  [0.25347 0.05762]]\n",
      "\n",
      " [[0.8357  0.03207]\n",
      "  [0.12337 0.00886]]\n",
      "\n",
      " [[0.69872 0.26034]\n",
      "  [0.02412 0.01682]]\n",
      "\n",
      " [[0.63701 0.08583]\n",
      "  [0.22134 0.05582]]]\n",
      "Your value of xi is: [[[0.64523 0.00601]\n",
      "  [0.34278 0.00598]]\n",
      "\n",
      " [[0.60684 0.38117]\n",
      "  [0.00551 0.00648]]\n",
      "\n",
      " [[0.40595 0.2064 ]\n",
      "  [0.19863 0.18902]]\n",
      "\n",
      " [[0.5718  0.03278]\n",
      "  [0.35711 0.03831]]\n",
      "\n",
      " [[0.02625 0.90266]\n",
      "  [0.00109 0.07   ]]\n",
      "\n",
      " [[0.02482 0.00251]\n",
      "  [0.81777 0.15489]]\n",
      "\n",
      " [[0.59943 0.24316]\n",
      "  [0.08947 0.06793]]\n",
      "\n",
      " [[0.6143  0.07461]\n",
      "  [0.25347 0.05762]]\n",
      "\n",
      " [[0.8357  0.03207]\n",
      "  [0.12337 0.00886]]\n",
      "\n",
      " [[0.69872 0.26034]\n",
      "  [0.02412 0.01682]]\n",
      "\n",
      " [[0.63701 0.08583]\n",
      "  [0.22134 0.05582]]]\n",
      "The value of gamma should be: [[0.65124 0.34876]\n",
      " [0.98802 0.01198]\n",
      " [0.61235 0.38765]\n",
      " [0.60458 0.39542]\n",
      " [0.92891 0.07109]\n",
      " [0.02733 0.97267]\n",
      " [0.8426  0.1574 ]\n",
      " [0.68891 0.31109]\n",
      " [0.86777 0.13223]\n",
      " [0.95906 0.04094]\n",
      " [0.72284 0.27716]\n",
      " [0.85835 0.14165]]\n",
      "Your value of gamma is: [[0.65124 0.34876]\n",
      " [0.98802 0.01198]\n",
      " [0.61235 0.38765]\n",
      " [0.60458 0.39542]\n",
      " [0.92891 0.07109]\n",
      " [0.02733 0.97267]\n",
      " [0.8426  0.1574 ]\n",
      " [0.68891 0.31109]\n",
      " [0.86777 0.13223]\n",
      " [0.95906 0.04094]\n",
      " [0.72284 0.27716]\n",
      " [0.85835 0.14165]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:00<00:01, 178.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -96.33989919755486\n",
      "log-likelihood -61.83737464303461\n",
      "log-likelihood -59.659076127854874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 36/200 [00:00<00:01, 141.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.78006168233149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 57/200 [00:00<00:00, 167.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.05638622904577\n",
      "log-likelihood -58.047573998672505\n",
      "log-likelihood -58.04756024248873\n",
      "log-likelihood -58.04756014666195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 78/200 [00:00<00:00, 179.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.04756014303574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 116/200 [00:00<00:00, 172.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.04756014156317\n",
      "log-likelihood -58.04756014077335\n",
      "log-likelihood -58.04756014034711\n",
      "log-likelihood -58.047560140117014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 152/200 [00:00<00:00, 163.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.04756013999281\n",
      "log-likelihood -58.04756013992577\n",
      "log-likelihood -58.047560139889605\n",
      "log-likelihood -58.04756013987006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 185/200 [00:01<00:00, 148.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.04756013985951\n",
      "log-likelihood -58.04756013985383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 156.77it/s]\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:165: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -58.047560139850745\n",
      "hmm.A should be\n",
      " [[0.      1.      0.      0.     ]\n",
      " [0.14122 0.      0.27099 0.58779]\n",
      " [0.20671 0.      0.      0.79329]\n",
      " [0.      0.90909 0.09091 0.     ]]\n",
      "Your implementation has hmm.A to be\n",
      " [[0.      1.      0.      0.     ]\n",
      " [0.14122 0.      0.27099 0.58779]\n",
      " [0.20671 0.      0.      0.79329]\n",
      " [0.      0.90909 0.09091 0.     ]]\n",
      "hmm.B should be\n",
      " [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.     ]\n",
      " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
      "  0.0625 ]\n",
      " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
      "  0.     ]\n",
      " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
      "  0.     ]]\n",
      "Your implementation has hmm.B to be\n",
      " [[0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.     ]\n",
      " [0.0625  0.      0.      0.5     0.      0.125   0.125   0.      0.125\n",
      "  0.0625 ]\n",
      " [0.      0.20671 0.      0.      0.79329 0.      0.      0.      0.\n",
      "  0.     ]\n",
      " [0.24667 0.      0.57555 0.      0.09556 0.      0.      0.      0.08222\n",
      "  0.     ]]\n",
      "hmm.pi should be\n",
      " [1 0 0 0]\n",
      "Your implementation has hmm.pi to be\n",
      " [1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 163.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -76.25384934681466\n",
      "After this test case, hmm.A should either be approximately, [[0 1]\n",
      " [1 0]]\n",
      "This is your current value of hmm.A:  [[0. 1.]\n",
      " [1. 0.]]\n",
      "After this test case, hmm.B should either be approximately, [[0.  0.  0.5 0.5]\n",
      " [0.5 0.5 0.  0. ]]  or it should be  [[0.5 0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.5]]\n",
      "This is your current value of hmm.B:  [[0.      0.      0.52174 0.47826]\n",
      " [0.54167 0.45833 0.      0.     ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -34.60328285171365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [00:00<00:00, 171.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -15.641575294952439\n",
      "log-likelihood -3.295836866004329\n",
      "log-likelihood -3.295836866004329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 44/100 [00:00<00:00, 213.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -3.295836866004329\n",
      "log-likelihood -3.295836866004329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 66/100 [00:00<00:00, 192.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -3.295836866004329\n",
      "log-likelihood -3.295836866004329\n",
      "log-likelihood -3.295836866004329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 194.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -3.295836866004329\n",
      "After this test case, hmm.A should be the identity matrix [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "This is your current value of hmm.A:  [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "After this test case, hmm.B should be some 3 by 3 permutation matrix\n",
      "This is your current value of hmm.B:  [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nNote: The end_to_end_test is not as robust due to it using random starts. Try\\nrunning the test case a few times to see if you get a good result at least a few\\ntimes before deciding that your code is buggy.\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_test()\n",
    "forward_test()\n",
    "backward_test()\n",
    "forward_backward_test()\n",
    "baum_welch_update_test()\n",
    "end_to_end_test()\n",
    "\n",
    "\"\"\"\n",
    "Note: The end_to_end_test is not as robust due to it using random starts. Try\n",
    "running the test case a few times to see if you get a good result at least a few\n",
    "times before deciding that your code is buggy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eF-l7WucpCBP"
   },
   "source": [
    "## Training\n",
    "\n",
    "Train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTWXUt15pDg4",
    "outputId": "67bdefd8-aa9f-41ec-a0e1-a16b6b76f8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:29<04:25, 29.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -2080580.523715684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:52<00:00, 29.26s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lab_util.Tokenizer()\n",
    "tokenizer.fit(train_reviews)\n",
    "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
    "hmm.learn_unsupervised(train_reviews_tk, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiCwE05xqXmI"
   },
   "source": [
    "Let's look at some of the words associated with each hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXhMoLUFqbn_",
    "outputId": "be80e14e-db9c-4103-adc7-fe365417da50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0\n",
      "<unk> 0.10023212929390081\n",
      "this 0.0433827326731613\n",
      "the 0.03725014605563821\n",
      "a 0.03179362869423959\n",
      ", 0.02082109161454085\n",
      "for 0.020590898894998565\n",
      "good 0.020082810860937703\n",
      "and 0.016531318952906177\n",
      "product 0.0114813134645327\n",
      "they 0.009881889632919943\n",
      "\n",
      "state 1\n",
      "i 0.16298938282834718\n",
      "the 0.06390542597628308\n",
      "<unk> 0.056050086816525894\n",
      "to 0.04847716468297169\n",
      ". 0.03922834858410636\n",
      "my 0.03872752485469792\n",
      "a 0.038548909276006946\n",
      "of 0.03258521708953154\n",
      "as 0.021354561554857217\n",
      "so 0.021047404979503345\n",
      "\n",
      "state 2\n",
      "to 0.05898058531827785\n",
      "the 0.056115127576407214\n",
      "i 0.04862654201327898\n",
      "br 0.040941926654810536\n",
      "<unk> 0.03991197617703921\n",
      "and 0.037248848696367715\n",
      "this 0.035172668491221006\n",
      "a 0.030509976001164958\n",
      "these 0.015664713994694796\n",
      "can 0.015574850641422581\n",
      "\n",
      "state 3\n",
      ". 0.19399288548204338\n",
      "and 0.0756687514057655\n",
      ", 0.06537039003334821\n",
      "but 0.04482976244472893\n",
      "to 0.02659694281161817\n",
      "of 0.025256737740377986\n",
      "! 0.02411126584661508\n",
      "for 0.023154175786008244\n",
      "is 0.022158567856156716\n",
      "i 0.018068004571433583\n",
      "\n",
      "state 4\n",
      "<unk> 0.16318194540383468\n",
      ". 0.07823952775272966\n",
      "the 0.06171025771401362\n",
      "in 0.03372261148325783\n",
      "of 0.029724141598779064\n",
      "and 0.028328449730979012\n",
      "is 0.024957843562735157\n",
      "a 0.02019818357320861\n",
      "it 0.0174812171625852\n",
      "was 0.010076745653158766\n",
      "\n",
      "state 5\n",
      "<unk> 0.05023422097342755\n",
      "it 0.04061527060661762\n",
      "have 0.02622714627768653\n",
      "i 0.023300260168101695\n",
      "br 0.023037676433757367\n",
      "you 0.018055827841036737\n",
      "would 0.018005215558479705\n",
      "much 0.014610355170192501\n",
      "the 0.01451206422655938\n",
      "am 0.013111137258462432\n",
      "\n",
      "state 6\n",
      "<unk> 0.14379702900828698\n",
      ". 0.12670353718979824\n",
      ", 0.07208509674575075\n",
      "the 0.043461940143358684\n",
      "and 0.026798217713128975\n",
      "of 0.017745093144363584\n",
      "that 0.016986403601524185\n",
      "to 0.01640264799195711\n",
      "was 0.015203789196475932\n",
      "it 0.014638937787280384\n",
      "\n",
      "state 7\n",
      ". 0.07680423421045761\n",
      "is 0.04601667667633205\n",
      ", 0.044581443428706825\n",
      "a 0.04295545785423709\n",
      "br 0.026787203391091887\n",
      "the 0.02421198830904195\n",
      "are 0.023505646633491504\n",
      "for 0.020807306575086247\n",
      "<unk> 0.019480412409916915\n",
      "not 0.019146817332285897\n",
      "\n",
      "state 8\n",
      "<unk> 0.09587606882872608\n",
      ", 0.08470186572046258\n",
      "the 0.0314884666094781\n",
      "it 0.020962050849319534\n",
      "is 0.018728441738379023\n",
      "a 0.013871379528656969\n",
      "that 0.013704946452071336\n",
      "for 0.011038722482934924\n",
      "in 0.00954695045470072\n",
      "! 0.00950430817564099\n",
      "\n",
      "state 9\n",
      "<unk> 0.04644724650085307\n",
      "it 0.04363842080429357\n",
      "a 0.043115656056496914\n",
      "this 0.03488339996973561\n",
      "in 0.028118042030305955\n",
      "br 0.02631999134240186\n",
      "the 0.02505065587834171\n",
      ". 0.017213790525609125\n",
      "i 0.015643337650774608\n",
      "for 0.012677673731935797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hmm.num_states):\n",
    "    most_probable = np.argsort(hmm.B[i, :])[-10:][::-1]\n",
    "    print(f\"state {i}\")\n",
    "    for o in most_probable:\n",
    "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAQ_PmASwdFz"
   },
   "source": [
    "We can also look at some samples from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tj1eT3s3wgFJ",
    "outputId": "1e45c277-14e5-449f-bd73-d9e67f12a79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when comes a serving but a snacking br . subscribe']\n",
      "['so will more long results individual cents about a bills']\n",
      "['peppermint <unk> a other these love at a dog as']\n",
      "['they was it for potato have delicious my problem kids']\n",
      "['to <unk> save the are . the of for in']\n",
      "['finally we needed and but and 12 off you that']\n",
      "['i its was i with it <unk> <unk> beans five']\n",
      "['then for the <unk> with and if one expected little']\n",
      "['a this 3 <unk> much <unk> it . , chips']\n",
      "['that this . , . i us <unk> , are']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9Qk9adNr7lQ"
   },
   "source": [
    "Finally, let's repeat the classification experiment from HW 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
    "\n",
    "(Warning! results may not be the same as in earlier versions of this experiment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mL6JQXLJspyA",
    "outputId": "f5d55764-0778-4120-d733-701782eebf30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm features, 3000 examples\n",
      "test accuracy 0.632\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.632"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(xs_featurized, ys):\n",
    "  import sklearn.linear_model\n",
    "  model = sklearn.linear_model.LogisticRegression()\n",
    "  model.fit(xs_featurized, ys)\n",
    "  return model\n",
    "\n",
    "def eval_model(model, xs_featurized, ys):\n",
    "  pred_ys = model.predict(xs_featurized)\n",
    "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
    "  return np.mean(pred_ys == ys)\n",
    "\n",
    "def training_experiment(name, featurizer, n_train):\n",
    "    print(f\"{name} features, {n_train} examples\")\n",
    "    train_xs = np.array([\n",
    "        hmm_featurizer(review) \n",
    "        for review in tokenizer.tokenize(train_reviews[:n_train])\n",
    "    ])\n",
    "    train_ys = train_labels[:n_train]\n",
    "    test_xs = np.array([\n",
    "        hmm_featurizer(review)\n",
    "        for review in tokenizer.tokenize(test_reviews)\n",
    "    ])\n",
    "    test_ys = test_labels\n",
    "    model = train_model(train_xs, train_ys)\n",
    "    ret = eval_model(model, test_xs, test_ys)\n",
    "    print()\n",
    "    return ret\n",
    "\n",
    "def hmm_featurizer(review):\n",
    "    _, _, gamma = hmm.forward_backward(review)\n",
    "    return gamma.sum(axis=0)\n",
    "\n",
    "training_experiment(\"hmm\", hmm_featurizer, n_train=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6DI4otm0YHe"
   },
   "source": [
    "## Experiments for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovbrCIUT0NGy",
    "outputId": "0e47f1df-5552-41ea-fe75-8ac613798f65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:28<04:18, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -2127651.6457295115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:28<00:00, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0\n",
      "<unk> 0.10110636453973805\n",
      "the 0.07763417468356273\n",
      "of 0.03252030601254949\n",
      "this 0.02140621785636538\n",
      "in 0.020651277926282772\n",
      "a 0.020391296547111873\n",
      ". 0.020221344982852024\n",
      "is 0.01950398792860412\n",
      "for 0.016271813282601855\n",
      "my 0.015581468546229034\n",
      "\n",
      "state 1\n",
      ". 0.1017658326124267\n",
      ", 0.05967216746136682\n",
      "<unk> 0.057618994124041235\n",
      "i 0.050553161183079585\n",
      "and 0.03409804782700789\n",
      "br 0.026907730366436036\n",
      "it 0.026674946752373876\n",
      "to 0.02574534067633097\n",
      "a 0.024737819163346025\n",
      "but 0.014357660924927303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(num_states=2, num_words=tokenizer.vocab_size)\n",
    "hmm.learn_unsupervised(train_reviews_tk, 10)\n",
    "for i in range(hmm.num_states):\n",
    "    most_probable = np.argsort(hmm.B[i, :])[-10:][::-1]\n",
    "    print(f\"state {i}\")\n",
    "    for o in most_probable:\n",
    "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Q_zTmfG2Q-m",
    "outputId": "2bf6772b-fdeb-4396-c81c-7fa387fbb580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:41<15:10, 101.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood -2074940.547309131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [18:16<00:00, 109.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0\n",
      ", 0.07206631887140244\n",
      ". 0.06669286331521215\n",
      "<unk> 0.0435086353916481\n",
      "the 0.04208082481610322\n",
      "and 0.04049085983915092\n",
      "br 0.024969525414182826\n",
      "it 0.022472758741178656\n",
      "is 0.017361466229866125\n",
      "was 0.012403350053935374\n",
      "in 0.012113405788285727\n",
      "\n",
      "state 1\n",
      "<unk> 0.08097768178583495\n",
      ", 0.05881321520290113\n",
      "the 0.05001117809337514\n",
      ". 0.04189703721000224\n",
      "to 0.036105753069762984\n",
      "of 0.03167242063347812\n",
      "and 0.030930919909646956\n",
      "is 0.025770383718221206\n",
      "it 0.02160256698726869\n",
      "a 0.0204840520074068\n",
      "\n",
      "state 2\n",
      ". 0.10885757894091\n",
      "<unk> 0.05451938228934304\n",
      "and 0.04390016072350719\n",
      "it 0.029365673720345722\n",
      "i 0.028310270976631698\n",
      "the 0.02522650188379479\n",
      "to 0.023810996393284992\n",
      ", 0.015099877613742343\n",
      "br 0.014964096407588688\n",
      "this 0.012382620556284661\n",
      "\n",
      "state 3\n",
      "the 0.08532052681963398\n",
      "<unk> 0.06255759504145368\n",
      "a 0.04762367205043438\n",
      ". 0.04577520991181272\n",
      ", 0.02805132220427902\n",
      "and 0.024579606178408316\n",
      "in 0.02158457573305356\n",
      "for 0.019687678390700833\n",
      "i 0.01745929131064213\n",
      "br 0.01644892448185306\n",
      "\n",
      "state 4\n",
      "<unk> 0.13615508462046821\n",
      ". 0.06089713635443354\n",
      "and 0.034418165155500245\n",
      "i 0.028237622058687132\n",
      "to 0.024906397955015112\n",
      "a 0.022849114535873242\n",
      ", 0.019751464575604925\n",
      "my 0.018863534582393794\n",
      "this 0.018011725097828737\n",
      "it 0.01652786712747153\n",
      "\n",
      "state 5\n",
      "a 0.04561331267087582\n",
      "have 0.03337769609673852\n",
      "and 0.030400476138560995\n",
      "is 0.0282733731088517\n",
      "am 0.022363513739795175\n",
      "love 0.017192850665903703\n",
      "in 0.015733738259747297\n",
      "the 0.015057113021284822\n",
      "like 0.015032036049374779\n",
      ", 0.014938259790197516\n",
      "\n",
      "state 6\n",
      ". 0.09152419274185412\n",
      ", 0.05366657050863062\n",
      "and 0.04878953337944205\n",
      "a 0.04559893304641779\n",
      "the 0.040766003341389204\n",
      "is 0.024103819715207152\n",
      "it 0.021016818282127855\n",
      "in 0.020863689413734177\n",
      "<unk> 0.02054878984528913\n",
      "are 0.017713294239478622\n",
      "\n",
      "state 7\n",
      "the 0.05431142261750048\n",
      ". 0.05005571345900049\n",
      "a 0.04462624627427529\n",
      "i 0.03672049920331803\n",
      "<unk> 0.03639972087870345\n",
      "to 0.03565818795611059\n",
      ", 0.03217294989733198\n",
      "it 0.03159216591000733\n",
      "br 0.02067713603057687\n",
      "my 0.017574655073386996\n",
      "\n",
      "state 8\n",
      "i 0.07489748693343892\n",
      ". 0.07287875375088335\n",
      "the 0.07141340681121745\n",
      "<unk> 0.07030981544578557\n",
      "this 0.03320393916455322\n",
      ", 0.03006792664169237\n",
      "and 0.02926839000210039\n",
      "it 0.02265267795859977\n",
      "of 0.014203952238954253\n",
      "! 0.0115905565134891\n",
      "\n",
      "state 9\n",
      "<unk> 0.07298247224766692\n",
      "the 0.05528926937874661\n",
      ", 0.045506874294491115\n",
      ". 0.041692436389587376\n",
      "to 0.02771543832836869\n",
      "it 0.023763891808148238\n",
      "of 0.021749147319994847\n",
      "is 0.020066683478805823\n",
      "a 0.019713848814057022\n",
      "br 0.017487106911078774\n",
      "\n",
      "state 10\n",
      ". 0.11757054953587681\n",
      "br 0.04830214206239368\n",
      "a 0.036327640388437894\n",
      "it 0.03598415557706175\n",
      "to 0.0354085754466078\n",
      "i 0.031461248890119704\n",
      "that 0.017871842441425607\n",
      "in 0.016950304838988156\n",
      ", 0.011192668016067088\n",
      "is 0.0105019169223384\n",
      "\n",
      "state 11\n",
      ". 0.12533138725310367\n",
      "<unk> 0.0599903435543674\n",
      ", 0.05335527448659743\n",
      "the 0.04125139161577122\n",
      "of 0.03128246345830094\n",
      "i 0.01808087557717272\n",
      "is 0.01777187506777602\n",
      "this 0.01630651522512129\n",
      "in 0.013412224779807769\n",
      "br 0.012475070519919652\n",
      "\n",
      "state 12\n",
      "<unk> 0.15458607437069777\n",
      "the 0.06767918579554864\n",
      ". 0.051797734149438376\n",
      "and 0.03139981890720004\n",
      ", 0.030456191171110224\n",
      "for 0.02028832247287299\n",
      "to 0.01963858901254291\n",
      "is 0.016239867789467288\n",
      "! 0.01567842175276325\n",
      "it 0.014715568497287342\n",
      "\n",
      "state 13\n",
      ". 0.09961864222701626\n",
      "<unk> 0.047128092712685775\n",
      "a 0.04312887969254019\n",
      ", 0.04194472498660805\n",
      "and 0.027270758880913612\n",
      "of 0.02631794199063096\n",
      "is 0.0232713090654659\n",
      "br 0.019877968953477473\n",
      "in 0.018167333109919175\n",
      "to 0.015985763466016868\n",
      "\n",
      "state 14\n",
      ". 0.10711757722241474\n",
      ", 0.06497534071387084\n",
      "i 0.05592679937289819\n",
      "<unk> 0.044645173744014006\n",
      "and 0.0336606107371732\n",
      "to 0.028583880911362807\n",
      "in 0.024386671773485098\n",
      "is 0.022129965929820576\n",
      "my 0.016364319693449553\n",
      "but 0.013320626681316327\n",
      "\n",
      "state 15\n",
      ". 0.09717040640281865\n",
      "<unk> 0.08310289928527508\n",
      ", 0.0561971458196018\n",
      "to 0.036308224569172556\n",
      "and 0.03490209413741654\n",
      "of 0.02951610173458725\n",
      "a 0.018546878903258943\n",
      "for 0.0177230835468845\n",
      "! 0.01401961116427668\n",
      "br 0.013191890979156896\n",
      "\n",
      "state 16\n",
      "the 0.06364551487538796\n",
      "i 0.04522495906452381\n",
      "<unk> 0.04074399929183815\n",
      ". 0.04054637837833505\n",
      "to 0.03363810569221888\n",
      ", 0.032581629513979735\n",
      "is 0.02222090816288038\n",
      "they 0.018147388556514873\n",
      "but 0.01733116112044371\n",
      "and 0.01698406192865958\n",
      "\n",
      "state 17\n",
      "<unk> 0.16508654845912674\n",
      ", 0.06047834203928537\n",
      "and 0.03976628306528445\n",
      "the 0.03169552445853287\n",
      "to 0.028510550107956164\n",
      "of 0.02290844115013464\n",
      ". 0.019072404595767686\n",
      "a 0.01667233331389234\n",
      "! 0.013264418086076953\n",
      "i 0.012595933644371062\n",
      "\n",
      "state 18\n",
      "<unk> 0.06648139877317424\n",
      ". 0.06076806548305945\n",
      "a 0.056306938934519296\n",
      "it 0.026764907355025205\n",
      "i 0.025023268678995673\n",
      ", 0.024777718381515594\n",
      "and 0.02103758541478589\n",
      "to 0.0193796851670239\n",
      "is 0.01487509490913362\n",
      "of 0.014790634052641603\n",
      "\n",
      "state 19\n",
      "<unk> 0.13624579506889603\n",
      ". 0.11488191258408237\n",
      "and 0.03104573154123369\n",
      "of 0.027363739449074807\n",
      "the 0.022084607728511688\n",
      ", 0.01989384150097605\n",
      "i 0.01789226581806071\n",
      "that 0.016056119667570496\n",
      "are 0.01335166184451212\n",
      "you 0.01211183083763704\n",
      "\n",
      "state 20\n",
      "i 0.11569560880059122\n",
      "the 0.08446854375542291\n",
      "this 0.0586409620055569\n",
      ". 0.043196537731762755\n",
      "my 0.02719119756524469\n",
      "<unk> 0.024746453790377824\n",
      "these 0.023170747036787176\n",
      "and 0.021915532442866103\n",
      "for 0.01343635955362074\n",
      "is 0.012287833289075706\n",
      "\n",
      "state 21\n",
      ". 0.0700753631761222\n",
      "i 0.046169442487607376\n",
      "the 0.04427690992347864\n",
      "<unk> 0.030960026953419366\n",
      "br 0.028709118066906302\n",
      "it 0.027981523063329596\n",
      ", 0.027219807152361726\n",
      "in 0.015267434370266116\n",
      "and 0.014547274072137784\n",
      "of 0.012969069825439031\n",
      "\n",
      "state 22\n",
      "<unk> 0.11762868588477378\n",
      ". 0.09022686729448043\n",
      "the 0.04616892288103652\n",
      ", 0.04301958335201837\n",
      "a 0.029282438551732872\n",
      "is 0.018625468335972707\n",
      "to 0.01856482958794575\n",
      "it 0.017865131993353094\n",
      "and 0.01315105795030428\n",
      "for 0.012335869226705416\n",
      "\n",
      "state 23\n",
      "<unk> 0.08910220412436536\n",
      "a 0.04572599503504999\n",
      "and 0.0403850701130006\n",
      ", 0.03499697111440357\n",
      "i 0.03330824199889053\n",
      "br 0.03031971958617405\n",
      "the 0.029996409685686797\n",
      ". 0.027932472185332603\n",
      "in 0.020776494734415143\n",
      "of 0.018997232966970316\n",
      "\n",
      "state 24\n",
      ". 0.12322553405916543\n",
      ", 0.06811220965793521\n",
      "and 0.05281518724279036\n",
      "it 0.03713863012136639\n",
      "of 0.024109619862514524\n",
      "for 0.02262070415997855\n",
      "is 0.02128602462210696\n",
      "this 0.016352536779993206\n",
      "! 0.014900261627089037\n",
      "that 0.014744813166016632\n",
      "\n",
      "state 25\n",
      "<unk> 0.07771282107441155\n",
      "a 0.04608572478386818\n",
      "the 0.036444246441923296\n",
      ". 0.034889899439294714\n",
      "i 0.021289452587235573\n",
      "to 0.019423142895633693\n",
      "and 0.018052001821419115\n",
      "br 0.016446681475456176\n",
      ", 0.013796462516450222\n",
      "for 0.01361795592595689\n",
      "\n",
      "state 26\n",
      "<unk> 0.1493538720060197\n",
      ". 0.0809841556636607\n",
      ", 0.05679318853454914\n",
      "the 0.05623798613583535\n",
      "to 0.028499416579182008\n",
      "of 0.02596710516197822\n",
      "and 0.02426191994442168\n",
      "it 0.022144559240313152\n",
      "i 0.018644924613007575\n",
      "a 0.017624767361712864\n",
      "\n",
      "state 27\n",
      "i 0.4068854157275654\n",
      "this 0.0694725064489462\n",
      "these 0.04376613719555774\n",
      "we 0.026712171129103915\n",
      "the 0.02459286931545201\n",
      "if 0.022257964817234792\n",
      "i've 0.022196463983922148\n",
      ". 0.022134100486770964\n",
      "<unk> 0.021121343248567594\n",
      "my 0.011495454168375855\n",
      "\n",
      "state 28\n",
      "the 0.05275626958728059\n",
      "a 0.04955237646803285\n",
      "i 0.03849463932585324\n",
      "and 0.03821015883009221\n",
      "to 0.035871530450126705\n",
      ", 0.031424766015742345\n",
      "it 0.03133472334803714\n",
      "of 0.02169681049214935\n",
      "<unk> 0.020854783011739004\n",
      "not 0.01760995412669319\n",
      "\n",
      "state 29\n",
      "this 0.10081015339032254\n",
      "i 0.08654676006391092\n",
      "the 0.07671294785557124\n",
      "<unk> 0.06276984693100099\n",
      "these 0.04074335217169083\n",
      "my 0.03493248935539948\n",
      "to 0.020703237347898272\n",
      "a 0.020265367348367955\n",
      "not 0.012260373716692002\n",
      "i'm 0.01084590899524485\n",
      "\n",
      "state 30\n",
      ". 0.0699332156903933\n",
      ", 0.058282180660799716\n",
      "to 0.035489168620424755\n",
      "the 0.03316687174517834\n",
      "i 0.0313288129415569\n",
      "and 0.024939604567194247\n",
      "for 0.023653407477395687\n",
      "<unk> 0.02298638098821979\n",
      "this 0.02094503723729542\n",
      "in 0.019152550634208208\n",
      "\n",
      "state 31\n",
      ". 0.07094668396165887\n",
      "<unk> 0.06964401935784703\n",
      ", 0.054514838585559715\n",
      "the 0.0484221619038139\n",
      "a 0.027576101247548706\n",
      "br 0.01992640255870143\n",
      "for 0.015776480196605686\n",
      "i 0.01480341910384988\n",
      "of 0.014715336237024403\n",
      "but 0.013595701419675036\n",
      "\n",
      "state 32\n",
      ". 0.08107953202269363\n",
      "<unk> 0.06231381389709646\n",
      ", 0.05515492425664199\n",
      "the 0.0446301413464738\n",
      "a 0.031667536946830804\n",
      "and 0.03014378724913869\n",
      "in 0.018666429750631906\n",
      "but 0.015557292677502044\n",
      "that 0.014803221688992087\n",
      "to 0.013481854888519591\n",
      "\n",
      "state 33\n",
      "<unk> 0.07964622937781689\n",
      ". 0.06936093029306047\n",
      "the 0.06486613242947349\n",
      ", 0.05197178557353825\n",
      "to 0.03475634148127843\n",
      "of 0.029762333870058347\n",
      "a 0.024524282885904348\n",
      "i 0.020210054952352122\n",
      "in 0.015585225721504616\n",
      "this 0.015315364896645416\n",
      "\n",
      "state 34\n",
      ". 0.04683372081489213\n",
      "and 0.037750215427115534\n",
      "the 0.029902959609356402\n",
      "this 0.023054592001115943\n",
      ", 0.019980399770396045\n",
      "<unk> 0.019589705384413743\n",
      "for 0.016525500340219705\n",
      "it 0.015563361881017035\n",
      "have 0.015283130839382712\n",
      "of 0.014636645742439074\n",
      "\n",
      "state 35\n",
      "<unk> 0.15988374404960848\n",
      "to 0.03543075461245642\n",
      ", 0.03352628330901274\n",
      "and 0.029964643565667266\n",
      "is 0.026647812061811743\n",
      ". 0.025859997822820696\n",
      "of 0.019638144027442857\n",
      "for 0.0166401561399814\n",
      "the 0.01620179803313706\n",
      "them 0.008961197682533662\n",
      "\n",
      "state 36\n",
      ". 0.09196991271815431\n",
      "<unk> 0.08550573182635557\n",
      ", 0.04820267751471352\n",
      "br 0.0361117511061144\n",
      "of 0.03161630889378952\n",
      "and 0.03136661693407019\n",
      "is 0.023777905814846578\n",
      "for 0.015494608370799294\n",
      "to 0.013965337759818132\n",
      "in 0.01374981397776057\n",
      "\n",
      "state 37\n",
      "<unk> 0.1233851967981996\n",
      "a 0.04239384617679653\n",
      "it 0.025743413615678744\n",
      "of 0.020156062725115616\n",
      "in 0.019941115504875404\n",
      "and 0.01925443649065182\n",
      "but 0.017753114267263528\n",
      ", 0.01698271079460983\n",
      "this 0.013284244740906049\n",
      "not 0.012896236376764212\n",
      "\n",
      "state 38\n",
      ". 0.10919131189211814\n",
      "the 0.07798060595237825\n",
      ", 0.061975843072018075\n",
      "<unk> 0.040368889762715444\n",
      "of 0.028505365872936325\n",
      "a 0.02593880946911487\n",
      "in 0.023976788587902587\n",
      "it 0.02362233305528101\n",
      "for 0.020256152615011867\n",
      "i 0.011048633265625824\n",
      "\n",
      "state 39\n",
      ". 0.10633509744486491\n",
      "<unk> 0.05578252572190233\n",
      "the 0.054038977292379234\n",
      "to 0.033868689546282166\n",
      "br 0.02428164210781189\n",
      "and 0.019937637090974\n",
      ", 0.01912215134632181\n",
      "in 0.017321644659747816\n",
      "it 0.013352375854801783\n",
      "was 0.01255405802843124\n",
      "\n",
      "state 40\n",
      ". 0.06897168673099807\n",
      "of 0.032862153993131527\n",
      ", 0.031539518130417464\n",
      "br 0.029786720304159686\n",
      "<unk> 0.023470425190886383\n",
      "is 0.02319065627894637\n",
      "that 0.01982872549643987\n",
      "was 0.015145009682378507\n",
      "and 0.014865494145603932\n",
      "you 0.014485375347039805\n",
      "\n",
      "state 41\n",
      "<unk> 0.14731109284017516\n",
      ". 0.04828389710416344\n",
      "and 0.04116912230604914\n",
      "a 0.038435692894140096\n",
      ", 0.03580664693426406\n",
      "the 0.02729808126047727\n",
      "to 0.024618297992984383\n",
      "it 0.0234609569472297\n",
      "is 0.021864064290842044\n",
      "of 0.0171540905662132\n",
      "\n",
      "state 42\n",
      "<unk> 0.11724751218395088\n",
      ". 0.09484298590221871\n",
      "the 0.07099657278051086\n",
      "to 0.036000957885791544\n",
      "of 0.017902122176481492\n",
      "is 0.014232083864319393\n",
      "! 0.013782429104597385\n",
      "it 0.013737946482875345\n",
      "for 0.013644584903038687\n",
      "but 0.011912346530811684\n",
      "\n",
      "state 43\n",
      "the 0.06338483510269492\n",
      ", 0.038711199682549026\n",
      ". 0.034925638177285764\n",
      "of 0.033793036617041264\n",
      "to 0.03349350873913653\n",
      "a 0.03298923883328117\n",
      "it 0.03187127911690519\n",
      "but 0.019480332456429897\n",
      "<unk> 0.019304317425605292\n",
      "and 0.015707312655956616\n",
      "\n",
      "state 44\n",
      ", 0.06935177834574938\n",
      "<unk> 0.06148965703197498\n",
      "it 0.024991451668793553\n",
      "and 0.02217134657923095\n",
      "of 0.02030217163424468\n",
      "i 0.019766528381446952\n",
      "not 0.01912982389998797\n",
      "to 0.019057671730393504\n",
      "this 0.01870516341811361\n",
      "is 0.018250227142856647\n",
      "\n",
      "state 45\n",
      ". 0.08769613938438735\n",
      "the 0.0556084771441772\n",
      "is 0.032276231067210746\n",
      "and 0.02534357242883771\n",
      "was 0.0224617491559312\n",
      "<unk> 0.022131268605289203\n",
      "i 0.020655270595469746\n",
      ", 0.019379943557825026\n",
      "not 0.016418515364330834\n",
      "with 0.013148608720420268\n",
      "\n",
      "state 46\n",
      "<unk> 0.14046854565222233\n",
      ". 0.0729070007035754\n",
      "and 0.0340181489580632\n",
      "to 0.03182847328222586\n",
      ", 0.025861630001150472\n",
      "the 0.023878362268406235\n",
      "it 0.018364228309723386\n",
      "for 0.0174944946803359\n",
      "not 0.01584521098919367\n",
      "of 0.015309205735478327\n",
      "\n",
      "state 47\n",
      ". 0.09221494920622435\n",
      "the 0.056814615206704146\n",
      ", 0.05375049688094064\n",
      "<unk> 0.0480153555654851\n",
      "it 0.020997189740460213\n",
      "to 0.020557380725804\n",
      "in 0.018094290781104493\n",
      "a 0.01716643276115978\n",
      "of 0.015591273160754888\n",
      "they 0.012083503958432633\n",
      "\n",
      "state 48\n",
      "<unk> 0.12014286299313956\n",
      ". 0.05379359658873543\n",
      "the 0.03742654421981853\n",
      "to 0.033559065943479044\n",
      "is 0.021399196465349345\n",
      "and 0.018263112222118308\n",
      "of 0.018014932058538193\n",
      "a 0.017679617264837813\n",
      "in 0.01353145176392381\n",
      ", 0.012693913921824297\n",
      "\n",
      "state 49\n",
      "<unk> 0.04227356904288501\n",
      "is 0.03197813517579939\n",
      "to 0.030693154750418943\n",
      "was 0.02988303520528131\n",
      "and 0.027284650597041785\n",
      ", 0.02695897945999225\n",
      "a 0.02468854788638517\n",
      "br 0.020756290419352473\n",
      "but 0.01902338871755947\n",
      "the 0.016891875841058918\n",
      "\n",
      "state 50\n",
      "<unk> 0.1360338431174326\n",
      ", 0.041582257268444944\n",
      "it 0.03275299893543804\n",
      "to 0.02513818851508995\n",
      "the 0.02148590699305977\n",
      "is 0.01942215008199159\n",
      "but 0.017656010637101864\n",
      "and 0.01314456071059318\n",
      "br 0.012894770842555742\n",
      "a 0.01126760159916266\n",
      "\n",
      "state 51\n",
      "<unk> 0.0925136614906951\n",
      ", 0.08845626876228951\n",
      "the 0.048087263270456086\n",
      "and 0.0371294588538566\n",
      "to 0.02358027154086599\n",
      "i 0.01864619960201094\n",
      "a 0.016339598427356418\n",
      "in 0.013959257190197407\n",
      ". 0.013635185689782454\n",
      "br 0.013384369859445359\n",
      "\n",
      "state 52\n",
      "<unk> 0.16576111670206098\n",
      ". 0.09305250760280584\n",
      ", 0.025400490541632164\n",
      "of 0.021139904422304116\n",
      "a 0.019998228573476155\n",
      "is 0.015417123284746042\n",
      "br 0.014396375850516185\n",
      "not 0.01293111283272535\n",
      "you 0.011705711344008492\n",
      "and 0.010529811389795043\n",
      "\n",
      "state 53\n",
      "<unk> 0.10263167470859663\n",
      ". 0.09590454798439468\n",
      "br 0.043286655363628086\n",
      "it 0.03175511195620544\n",
      "a 0.028210846323352443\n",
      "of 0.02537770324204309\n",
      ", 0.018464251148044005\n",
      "i 0.015214340753028457\n",
      "the 0.01479514816240471\n",
      "! 0.012783865967673474\n",
      "\n",
      "state 54\n",
      "the 0.08063154134823125\n",
      "and 0.047206627051561825\n",
      ", 0.04061912455339801\n",
      "a 0.03981648631856496\n",
      "<unk> 0.030114834060515114\n",
      "is 0.0280678772568504\n",
      "it 0.026340275123672464\n",
      "br 0.024586126899695908\n",
      "this 0.020736064057486302\n",
      "for 0.01978077117179492\n",
      "\n",
      "state 55\n",
      ". 0.10841238270331825\n",
      ", 0.06174934311860765\n",
      "<unk> 0.049265411783379565\n",
      "a 0.047501235063661586\n",
      "the 0.02910511937678178\n",
      "it 0.029034141511377107\n",
      "and 0.02731316192775836\n",
      "to 0.01856016246162\n",
      "for 0.016113374562976714\n",
      "not 0.013122471493067391\n",
      "\n",
      "state 56\n",
      ". 0.1011014813103379\n",
      "<unk> 0.07282982105401103\n",
      "and 0.04464893368957901\n",
      "br 0.02960128648826589\n",
      ", 0.026309647055442247\n",
      "to 0.017640053747081517\n",
      "a 0.017458560309636117\n",
      "the 0.01718212549038883\n",
      "! 0.015964421686592553\n",
      "i 0.01488224445803999\n",
      "\n",
      "state 57\n",
      "<unk> 0.14115800242684723\n",
      ". 0.07090200146594569\n",
      "the 0.04221705954198842\n",
      "a 0.03869449470421507\n",
      "and 0.02990146334031707\n",
      ", 0.02891985754554391\n",
      "to 0.026009316299116327\n",
      "it 0.016101739069757492\n",
      "br 0.013409549030960258\n",
      "i 0.01237728420172458\n",
      "\n",
      "state 58\n",
      "i 0.1239148981626001\n",
      "the 0.04970902179495418\n",
      "this 0.04151046923646859\n",
      "my 0.03768614465021017\n",
      "to 0.030630824359189686\n",
      "these 0.025858816563140336\n",
      ", 0.01918494052978994\n",
      "and 0.016524558060245546\n",
      "a 0.01643276832201982\n",
      "in 0.015178106052373415\n",
      "\n",
      "state 59\n",
      "<unk> 0.10592667187802005\n",
      "the 0.07539778157727106\n",
      "a 0.036747654505108696\n",
      "and 0.02438921057090998\n",
      ", 0.023482799344720154\n",
      "i 0.022741901523591658\n",
      "to 0.021286077078970993\n",
      "in 0.019406487438157285\n",
      "this 0.018707217374637805\n",
      "it 0.017928617451525845\n",
      "\n",
      "state 60\n",
      "<unk> 0.11709473955158858\n",
      "the 0.07309340281517245\n",
      ", 0.050069751343453435\n",
      "and 0.03103358705957866\n",
      "br 0.020802773852833873\n",
      "it 0.0187564854957172\n",
      "in 0.01689020859974551\n",
      "not 0.01575447284408011\n",
      "this 0.014046762811485636\n",
      "that 0.013523910670924368\n",
      "\n",
      "state 61\n",
      "<unk> 0.1612257846993859\n",
      ". 0.07082049092630774\n",
      "the 0.032819779665951225\n",
      "of 0.028251059871864687\n",
      "to 0.026169253213578558\n",
      "is 0.02558921108981119\n",
      ", 0.01947690540900545\n",
      "a 0.018271981945253882\n",
      "for 0.01749960189597549\n",
      "and 0.011799464641680734\n",
      "\n",
      "state 62\n",
      ". 0.055897088450480706\n",
      "and 0.054325212133858276\n",
      "to 0.03861740689306727\n",
      "is 0.035932803890643995\n",
      "the 0.0357814272602806\n",
      "a 0.035091477105684686\n",
      "<unk> 0.023715285415552633\n",
      ", 0.02071547844904356\n",
      "i 0.01540668596496182\n",
      "for 0.014473804214028142\n",
      "\n",
      "state 63\n",
      "i 0.055291745000764926\n",
      ". 0.03295466124289208\n",
      "the 0.02208561845359786\n",
      "of 0.02067266948570344\n",
      "br 0.019015338848710053\n",
      "have 0.018896399492494532\n",
      "to 0.018310371758913712\n",
      "was 0.01790769942366993\n",
      "and 0.017456765510400457\n",
      "<unk> 0.01521248261463175\n",
      "\n",
      "state 64\n",
      "a 0.04665061245978801\n",
      ". 0.033850851219922186\n",
      "of 0.03303499158043755\n",
      "and 0.027854609977952756\n",
      "br 0.026936956980193823\n",
      "for 0.021130094753900514\n",
      "is 0.016748640869387314\n",
      ", 0.01591652057478068\n",
      "<unk> 0.015696142298984462\n",
      "! 0.014309662802154356\n",
      "\n",
      "state 65\n",
      "<unk> 0.15848850808236004\n",
      ". 0.0651846948616781\n",
      "the 0.04541055882425219\n",
      "and 0.03215204329708055\n",
      "a 0.02748380734811016\n",
      "it 0.020620562298629803\n",
      ", 0.017889044320073454\n",
      "that 0.015669479097681998\n",
      "in 0.014288181845456065\n",
      "have 0.013860550114967117\n",
      "\n",
      "state 66\n",
      "<unk> 0.08169979209079767\n",
      ", 0.05380889005385884\n",
      "to 0.03757857566107242\n",
      ". 0.03621684094200322\n",
      "a 0.0350562719280123\n",
      "this 0.017667095731439125\n",
      "for 0.015964546314817765\n",
      "but 0.01454586790496354\n",
      "! 0.014354497370303943\n",
      "that 0.014344839584382478\n",
      "\n",
      "state 67\n",
      ", 0.06065220505531598\n",
      "br 0.04593412879971778\n",
      "of 0.039060782372112475\n",
      "and 0.030542502526091312\n",
      "it 0.024175811700247084\n",
      "to 0.021279400450681378\n",
      "i 0.020593646389746357\n",
      "for 0.019121180579604276\n",
      "that 0.01810873644503145\n",
      "in 0.01781541541678776\n",
      "\n",
      "state 68\n",
      ". 0.11668231124815273\n",
      ", 0.04323978711991061\n",
      "<unk> 0.033950541677639744\n",
      "it 0.03305984091128627\n",
      "the 0.03040954843483406\n",
      "to 0.021696097524355768\n",
      "of 0.01939423437568302\n",
      "in 0.018458218619131755\n",
      "this 0.015430083684298533\n",
      "was 0.013894208227192046\n",
      "\n",
      "state 69\n",
      ". 0.12468545147410778\n",
      "<unk> 0.11426190095964073\n",
      "to 0.028981225021527588\n",
      "the 0.02442335020169514\n",
      "but 0.02189672279925066\n",
      "a 0.020527428886851988\n",
      "of 0.01975466340365195\n",
      "and 0.01507532384064238\n",
      "i 0.014968424182972825\n",
      "that 0.014070428397932065\n",
      "\n",
      "state 70\n",
      ". 0.10077169896701968\n",
      "<unk> 0.06785521775674835\n",
      "the 0.050905295100836995\n",
      ", 0.04706721959124668\n",
      "and 0.04263835141140333\n",
      "it 0.03051167620709257\n",
      "for 0.01831379947489877\n",
      "of 0.017331296197200193\n",
      "to 0.017254994616005763\n",
      "! 0.017033157100710482\n",
      "\n",
      "state 71\n",
      ". 0.12918312173812524\n",
      ", 0.06365100383661401\n",
      "and 0.03247754704880881\n",
      "br 0.024491253714447776\n",
      "a 0.02244765219905164\n",
      "i 0.021514531611605006\n",
      "<unk> 0.016299014323222814\n",
      "for 0.016207573350786528\n",
      "this 0.015146848925778686\n",
      "! 0.015085535920956896\n",
      "\n",
      "state 72\n",
      "the 0.06721023190081821\n",
      "<unk> 0.05964081066754963\n",
      "and 0.03902255963239376\n",
      ". 0.029294815159218855\n",
      "i 0.026884988252233365\n",
      "it 0.017902223632743908\n",
      "this 0.01695075157093536\n",
      ", 0.01655750903161445\n",
      "br 0.016434338347850137\n",
      "have 0.015749840435230554\n",
      "\n",
      "state 73\n",
      "<unk> 0.1197621701001795\n",
      "and 0.04213815030500154\n",
      "to 0.03844785943441558\n",
      "a 0.03654339959179326\n",
      "i 0.03570254493700564\n",
      ", 0.024997871859418104\n",
      "this 0.02323508201333831\n",
      "of 0.023073163672587858\n",
      "is 0.01482401078718158\n",
      "that 0.014451938147049757\n",
      "\n",
      "state 74\n",
      "<unk> 0.1502821879541341\n",
      ", 0.0372211071870003\n",
      "and 0.037027996927498474\n",
      "to 0.029732865770645624\n",
      "it 0.029687618577815923\n",
      "a 0.029347846257244598\n",
      "the 0.027337477412143357\n",
      ". 0.02695775757632026\n",
      "of 0.023519991326488385\n",
      "i 0.023409838140472476\n",
      "\n",
      "state 75\n",
      "<unk> 0.07654746351641811\n",
      ", 0.048210060246898286\n",
      "of 0.03198706398900573\n",
      "the 0.030929389351992364\n",
      "and 0.030085991523986794\n",
      "is 0.025727485411650947\n",
      "it 0.01798412489960369\n",
      "a 0.017241943623918506\n",
      "br 0.01496891524294465\n",
      ". 0.01387175107669273\n",
      "\n",
      "state 76\n",
      ". 0.08598133082728608\n",
      ", 0.06971497418243544\n",
      "i 0.04171524447231097\n",
      "a 0.0379530500949836\n",
      "and 0.03283921588228149\n",
      "<unk> 0.028883315014068378\n",
      "of 0.02788527206956543\n",
      "this 0.021908972722775526\n",
      "is 0.01608787841934584\n",
      "my 0.01603964591161073\n",
      "\n",
      "state 77\n",
      "the 0.07011440896222856\n",
      ". 0.06700994921906793\n",
      "<unk> 0.06246550280003993\n",
      ", 0.05269108435176053\n",
      "br 0.04394367412944539\n",
      "and 0.03794896756769645\n",
      "this 0.026516706815927035\n",
      "to 0.018926548135393503\n",
      "is 0.014953863165605256\n",
      "i 0.014854269566231667\n",
      "\n",
      "state 78\n",
      "i 0.11354250043721946\n",
      "<unk> 0.06672090791070205\n",
      "the 0.06555009708304205\n",
      ", 0.03138670365358677\n",
      ". 0.028210824824282235\n",
      "it 0.02328308250734305\n",
      "this 0.017886655903138662\n",
      "a 0.017631144238143708\n",
      "of 0.01747171526497996\n",
      "these 0.013802533025153486\n",
      "\n",
      "state 79\n",
      "<unk> 0.15782782022726805\n",
      ". 0.06368787301512573\n",
      "and 0.03525659643821214\n",
      "the 0.03400986321771196\n",
      ", 0.0336972749417715\n",
      "is 0.024337407317914048\n",
      "i 0.019804958788223432\n",
      "was 0.018392847690933172\n",
      "this 0.015409134443466064\n",
      "they 0.012626307028701366\n",
      "\n",
      "state 80\n",
      "the 0.050200053508023715\n",
      ", 0.043212989613774996\n",
      "<unk> 0.03906082680958056\n",
      "and 0.0351080039412307\n",
      ". 0.03197081695680644\n",
      "of 0.027809602946736806\n",
      "to 0.027704417034716787\n",
      "br 0.026321004349804843\n",
      "it 0.025756070794049853\n",
      "i 0.021078791947034846\n",
      "\n",
      "state 81\n",
      "<unk> 0.1639026007966734\n",
      ", 0.03296922686493035\n",
      "the 0.02983752261824525\n",
      "a 0.028296690347652746\n",
      "of 0.024199519068838323\n",
      "to 0.018231231334260516\n",
      "it 0.015119238812386904\n",
      "are 0.014873958952366848\n",
      "for 0.014724399052340832\n",
      "! 0.013896288460527907\n",
      "\n",
      "state 82\n",
      "<unk> 0.15281623280371973\n",
      "the 0.05937020981932399\n",
      ". 0.05858710490056005\n",
      "a 0.03040591773538784\n",
      "i 0.02199881721014493\n",
      "it 0.015709485661528313\n",
      ", 0.013210540055823465\n",
      "and 0.013193592328398684\n",
      "! 0.012358514160664436\n",
      "but 0.011274513837999994\n",
      "\n",
      "state 83\n",
      "i 0.12721837556574533\n",
      ". 0.08331872384206611\n",
      ", 0.048550203298926835\n",
      "a 0.034343165689957114\n",
      "it 0.03292634476407579\n",
      "and 0.03137657357311143\n",
      "br 0.024645905428106542\n",
      "my 0.021806052237522713\n",
      "the 0.021771933056053423\n",
      "if 0.01503956348371812\n",
      "\n",
      "state 84\n",
      "<unk> 0.15672171838416413\n",
      ". 0.08770252402804478\n",
      "a 0.03278782030831068\n",
      ", 0.03202774760833557\n",
      "i 0.025446741501634368\n",
      "of 0.02075332338660534\n",
      "in 0.018082544354501632\n",
      "is 0.015463064819124852\n",
      "the 0.015300016633603399\n",
      "that 0.013262936965268243\n",
      "\n",
      "state 85\n",
      ". 0.09826350772264408\n",
      "the 0.062438391690920164\n",
      "<unk> 0.045024316666693265\n",
      "a 0.03901367422552477\n",
      "and 0.03367862985702073\n",
      "i 0.030353669668582463\n",
      ", 0.027183220071193306\n",
      "it 0.020731884683694805\n",
      "of 0.013104522794510973\n",
      "are 0.012855847538408719\n",
      "\n",
      "state 86\n",
      "<unk> 0.14532861699732985\n",
      ". 0.04037136848498563\n",
      "a 0.03421607831828171\n",
      ", 0.023616794059450243\n",
      "to 0.023087430403278472\n",
      "is 0.021242517445200174\n",
      "of 0.02047506349369335\n",
      "this 0.018528064057008364\n",
      "that 0.014888276703464279\n",
      "was 0.013531894975407627\n",
      "\n",
      "state 87\n",
      ". 0.11107086434203878\n",
      "<unk> 0.07588475637974408\n",
      "the 0.04644657839603796\n",
      "a 0.034903276134900935\n",
      "is 0.029481776811165296\n",
      "it 0.02225066834579737\n",
      "br 0.01960766275363735\n",
      "to 0.018659217733896134\n",
      "not 0.010113193563305204\n",
      "and 0.009403885710490506\n",
      "\n",
      "state 88\n",
      "i 0.14382885137461418\n",
      "the 0.0706881484268333\n",
      "<unk> 0.06259599661375242\n",
      ". 0.06102465807693475\n",
      "this 0.035780324162191825\n",
      "my 0.028623895877843\n",
      ", 0.024287708130592573\n",
      "if 0.019513756618409504\n",
      "a 0.018459776767757556\n",
      "it 0.016659330075273766\n",
      "\n",
      "state 89\n",
      "<unk> 0.14817639259246806\n",
      ", 0.06645928062252368\n",
      ". 0.05064989677119438\n",
      "and 0.0360104378598461\n",
      "a 0.025068988306968364\n",
      "it 0.022826267877120276\n",
      "br 0.0195271159387098\n",
      "is 0.01941627757648672\n",
      "of 0.019224371802631184\n",
      "the 0.017271996870861367\n",
      "\n",
      "state 90\n",
      "<unk> 0.1261689403945044\n",
      ". 0.06700802153796963\n",
      ", 0.05902071303732405\n",
      "i 0.041426038061153636\n",
      "it 0.031875178713303004\n",
      "to 0.019950926351202797\n",
      "br 0.019203102237863125\n",
      "in 0.01869486478574369\n",
      "is 0.017946979447725345\n",
      "this 0.017638991185167863\n",
      "\n",
      "state 91\n",
      ". 0.08877654487779932\n",
      "i 0.05960958724478763\n",
      "a 0.035051820871407825\n",
      "is 0.028461559184111453\n",
      "to 0.02437797632817922\n",
      "<unk> 0.0236934701753879\n",
      ", 0.02236340098031279\n",
      "and 0.020919018553174334\n",
      "it 0.017591815845794074\n",
      "this 0.016534297293770043\n",
      "\n",
      "state 92\n",
      ", 0.04609094380663232\n",
      "a 0.04078829550479651\n",
      "it 0.03632681855156657\n",
      ". 0.03590800056016316\n",
      "to 0.031936738265476085\n",
      "of 0.021609227615571682\n",
      "<unk> 0.018932023134268384\n",
      "that 0.01825982295456773\n",
      "have 0.017684208970199036\n",
      "not 0.017034446350079217\n",
      "\n",
      "state 93\n",
      "<unk> 0.14426550046063544\n",
      ". 0.06956688238697986\n",
      "a 0.03911411022043017\n",
      "of 0.02451942701659287\n",
      "it 0.023128659458424357\n",
      "to 0.0204378240883087\n",
      "br 0.01799335445222505\n",
      "for 0.016753772584575017\n",
      "the 0.014993408481079454\n",
      "is 0.014551110534096114\n",
      "\n",
      "state 94\n",
      "<unk> 0.16737198830360048\n",
      ". 0.08442982555110273\n",
      "the 0.056858844080015805\n",
      "i 0.0435646625053012\n",
      "is 0.02019832549440086\n",
      "of 0.018467561804856193\n",
      "to 0.01199872065355274\n",
      "this 0.01195009708662253\n",
      "it 0.01173727022724009\n",
      "and 0.011641759723960688\n",
      "\n",
      "state 95\n",
      ". 0.14736932411248232\n",
      "and 0.0403589444255913\n",
      "is 0.030029693581247238\n",
      "a 0.027089824951569707\n",
      "the 0.026404605577689838\n",
      "br 0.023972247382191014\n",
      "but 0.022855409911576977\n",
      "are 0.015686723562982233\n",
      "<unk> 0.015496257038409672\n",
      "in 0.014104085435068673\n",
      "\n",
      "state 96\n",
      "<unk> 0.13810438363299404\n",
      ". 0.040247028182863714\n",
      "to 0.028033476138472447\n",
      "it 0.0256717532661048\n",
      ", 0.02367888929710613\n",
      "a 0.021948987742233788\n",
      "this 0.021426528789976803\n",
      "are 0.01753967411355921\n",
      "in 0.017036991243540165\n",
      "they 0.016767099896792162\n",
      "\n",
      "state 97\n",
      ". 0.10302147370756433\n",
      "the 0.051055133611088\n",
      "and 0.0417177665196828\n",
      "to 0.03309226256253609\n",
      "<unk> 0.03275731100875493\n",
      "br 0.026300259529907694\n",
      "it 0.02079808151948229\n",
      "is 0.019203300955214393\n",
      "but 0.013825928801963506\n",
      "of 0.013231780754687977\n",
      "\n",
      "state 98\n",
      "<unk> 0.14961249685171404\n",
      "the 0.0712103760581952\n",
      ", 0.05069408048408918\n",
      "i 0.03319714550004323\n",
      "of 0.028496637464530544\n",
      "it 0.02202571879737523\n",
      "and 0.020991141690847318\n",
      "my 0.016106541572952646\n",
      "is 0.01593607520937619\n",
      "br 0.015721637560562832\n",
      "\n",
      "state 99\n",
      ". 0.10528015533644414\n",
      "the 0.07336363539042014\n",
      "i 0.05329277304927196\n",
      "and 0.035162363229727\n",
      "of 0.027404980719322958\n",
      "to 0.022578567664727246\n",
      "this 0.02234482022333167\n",
      "it 0.02206079147126409\n",
      "<unk> 0.020698933637344943\n",
      "br 0.020568711596527643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(num_states=100, num_words=tokenizer.vocab_size)\n",
    "hmm.learn_unsupervised(train_reviews_tk, 10)\n",
    "for i in range(hmm.num_states):\n",
    "    most_probable = np.argsort(hmm.B[i, :])[-10:][::-1]\n",
    "    print(f\"state {i}\")\n",
    "    for o in most_probable:\n",
    "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmtZajmvGgBr",
    "outputId": "c5b0403b-78e2-4bdb-ddae-f3828793df2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm features, 250 examples\n",
      "test accuracy 0.588\n",
      "\n",
      "hmm features, 500 examples\n",
      "test accuracy 0.616\n",
      "\n",
      "hmm features, 750 examples\n",
      "test accuracy 0.628\n",
      "\n",
      "hmm features, 1000 examples\n",
      "test accuracy 0.61\n",
      "\n",
      "hmm features, 1250 examples\n",
      "test accuracy 0.648\n",
      "\n",
      "hmm features, 1500 examples\n",
      "test accuracy 0.614\n",
      "\n",
      "hmm features, 1750 examples\n",
      "test accuracy 0.616\n",
      "\n",
      "hmm features, 2000 examples\n",
      "test accuracy 0.61\n",
      "\n",
      "hmm features, 2250 examples\n",
      "test accuracy 0.612\n",
      "\n",
      "hmm features, 2500 examples\n",
      "test accuracy 0.622\n",
      "\n",
      "hmm features, 2750 examples\n",
      "test accuracy 0.624\n",
      "\n",
      "hmm features, 3000 examples\n",
      "test accuracy 0.632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_train_arr = list(range(250, 3001, 250))\n",
    "accuracies = []\n",
    "for n_train_i in n_train_arr:\n",
    "  acc = training_experiment(\"hmm\", hmm_featurizer, n_train=n_train_i)\n",
    "  accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "HaCUiKo1HV4K",
    "outputId": "db58c4b8-be3d-41ca-dcc2-6d0a4fff85cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.588, 0.616, 0.628, 0.61, 0.648, 0.614, 0.616, 0.61, 0.612, 0.622, 0.624, 0.632]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Test accuracy')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(6)\n",
    "fig.set_figwidth(8)\n",
    "fig.set_dpi(100)\n",
    "\n",
    "plt.plot(n_train_arr, accuracies)\n",
    "print(accuracies)\n",
    "\n",
    "plt.xlabel(\"Number of labeled samples\")\n",
    "plt.ylabel(\"Test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVDXZ2zVD2Jg"
   },
   "source": [
    "# Part 3: Trees\n",
    "In this part of the homework, we're moving on from HMMs (Part 2) to using trees. We will practice *parsing* on sentences from a semantic parsing corpus.  \n",
    "\n",
    "The data is obtained from this [paper](https://arxiv.org/pdf/1810.07942.pdf) (see Figure 1). As you can see from the figure, the purpose of this task is to understand what are the users *intents* from a query in plain text.  \n",
    "\n",
    "The end goal is that given sentence to decode a binary **tree structure** with *semantic tags* as *nodes*. For example:\n",
    "\n",
    "> whats there to do this weekend -> [<font color='00b8d4'>IN:GET_EVENT</font> whats there to do [<font color='00b8d4'>SL:DATE_TIME</font> this weekend]]  \n",
    "\n",
    "Note that the brackets [<font color='00b8d4'>LABEL</font> a substring of the text] indicates that this span is a sub-tree and <font color='00b8d4'>LABEL</font>  is the semantic label of the root of the sub-tree. You might read more about bracket representation in this [tutorial](https://www.tutorialspoint.com/binary-tree-to-string-with-brackets-in-cplusplus). \n",
    "\n",
    "1. In **Part B**, we formulate this problem as a simple classification problem --- the input to the classifier will be `(text, span)` and the output will be the semantic `label` of that span. `span`  is represented by two integer `(i,j)` which are the start and the end locations of the span.\n",
    "\n",
    "2. In **Part C**, we will implement a **CKY**-style decoding algorithm to decode the final tree based on the classifier we trained in Part B.\n",
    "\n",
    "We did pre-processing to enable CKY-style decoding for you. This includes binarization of the trees and handling of unary rules. (see the [code](https://github.mit.edu/tianxing/mit_6864_hw3_202003)).  \n",
    "\n",
    "Let's start by loading some dependencies and downloading the data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "C677T8MlVTqa"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"hw3/trees/\")\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from span_tree import *\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if cuda.is_available():\n",
    "  device = 'cuda'\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "else:\n",
    "  print('WARNING: you are running this assignment on a cpu!')\n",
    "  device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWM2L1dkVfa3"
   },
   "source": [
    "## **Agenda**\n",
    "\n",
    "We apply a model that learns the parsing structures in 4 steps.\n",
    "\n",
    "1. Enumerating all possible spans of a sentence\n",
    "2. Generating word and span embeddings\n",
    "3. Learning span label classifications\n",
    "4. Decoding a tree structure using the classification distributions of spans\n",
    "\n",
    "We go through this process step by step through the homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip1CRhe6t1FN"
   },
   "source": [
    "## **PART B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFigO36YuUJw"
   },
   "source": [
    "### **Data Processing**\n",
    "\n",
    "The very first step of the project is to load the corpus, building the **vocabulary**, **span label set**, and **span indices**. \n",
    "\n",
    "We first need to enumerate every node of a tree with a Depth First Search (DFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0n6rzUjMuXEP"
   },
   "outputs": [],
   "source": [
    "def tree_dfs(node, span_list, label_dict, mode):\n",
    "    \"\"\"\n",
    "    The base function for the recursion:\n",
    "      node: current root while traversing the tree\n",
    "      span_list: keep tracks of the spans and their label encodings in the tree e.g [[(0,1), 1], [(0,6),45] ...] \n",
    "      label_dict: mapping from label to their encodings e.g {\"UNK\":0, \"Token\":1,\"None\":2, ... }\n",
    "      mode: \"train\" or \"eval\"\n",
    "    \"\"\"\n",
    "  \n",
    "    if len(node.children) == 0:\n",
    "        assert(type(node) == Token)\n",
    "        cur_span = (node.index, node.index + 1)\n",
    "        cur_label = label_dict['Token']\n",
    "        span_list.append([cur_span, cur_label])\n",
    "        return span_list, label_dict\n",
    "        \n",
    "    cur_span = node.get_token_span()\n",
    "    cur_label = node.label\n",
    "    if node.label in label_dict:\n",
    "        cur_label = label_dict[node.label]\n",
    "    elif mode == 'train': # we are constructing the label dictionary\n",
    "        cur_label = len(label_dict)\n",
    "        label_dict[node.label] = cur_label\n",
    "    else:\n",
    "        cur_label = label_dict['UNK']\n",
    "    span_list.append([cur_span, cur_label])\n",
    "    \n",
    "    if len(node.children) > 1: #if only has one child, we will ignore the Token label, otherwise the token span would have two conflicting labels\n",
    "        for child in node.children:\n",
    "            # --------- Your code (hint: only need one single line) --------- #\n",
    "            tree_dfs(child, span_list, label_dict, mode)\n",
    "            # --------- Your code ends --------- # \n",
    "    return span_list, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIIcXk3Fuafv"
   },
   "source": [
    "Now, we go through the corpus and construct the **vocab dictionary** and the **label dictionary**. Note that we just add new words and labels to the dictionaries while building the training set. Unseen words or labels in validation and test set are marked as unknown (UNK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FcgswNogucno"
   },
   "outputs": [],
   "source": [
    "def process_line(line, vocab_dict, label_dict, mode):\n",
    "    '''\n",
    "    Processing a line in the corpus.\n",
    "    line format: Sentence \\t Sentence_Tree \\n\n",
    "    \n",
    "    Example:\n",
    "        'what is the shortest way home\\t\n",
    "        [IN:GET_DIRECTIONS what [SUB is [SUB the [SUB shortest [SUB way [SL:DESTINATION home ] ] ] ] ] ]\\n'\n",
    "    \n",
    "    Inputs:\n",
    "    vocab_dict: vocab dictionary {word: word_index, ...}\n",
    "    labels_dict: label dictionary {label: label_index, ...}\n",
    "    mode: \"train\" or \"eval\"\n",
    "    '''\n",
    "    s, s_tree = line.strip().split('\\t')\n",
    "    words = s.split(' ')\n",
    "    word_ids = []\n",
    "    for word in words:\n",
    "        if word in vocab_dict:\n",
    "            word_ids.append(vocab_dict[word])\n",
    "        elif mode == 'train':\n",
    "            word_ids.append(len(vocab_dict))\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "        else:\n",
    "            word_ids.append(vocab_dict['UNK'])\n",
    "    \n",
    "    tree = Tree(s_tree)\n",
    "    span_list = []\n",
    "    span_list, label_dict = tree_dfs(tree.root.children[0], span_list, label_dict, mode)\n",
    "    return word_ids, span_list, vocab_dict, label_dict\n",
    "\n",
    "def process_corpus(corpus_path, mode, vocab_dict=None, label_dict=None):\n",
    "    lines = open(corpus_path).readlines()\n",
    "    if not vocab_dict:\n",
    "        vocab_dict = {'UNK': 0}\n",
    "    if not label_dict:\n",
    "        label_dict = {'UNK': 0, 'Token': 1, 'None': 2}\n",
    "    corpus = []\n",
    "    sent_spans = []\n",
    "    raw_lines = []\n",
    "    for line in lines:\n",
    "      if len(line.strip()) < 3: \n",
    "        continue\n",
    "      word_ids, span_list, vocab_dict, label_dict = process_line(line, vocab_dict, label_dict, mode)\n",
    "      corpus.append(word_ids)\n",
    "      sent_spans.append(span_list)\n",
    "      raw_lines.append(line)\n",
    "    return corpus, sent_spans, vocab_dict, label_dict, raw_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yzl_Cq6ZufBr"
   },
   "outputs": [],
   "source": [
    "corpus_train, spans_train, vocab_dict, label_dict, train_lines = process_corpus('/content/hw3/trees/train.txt', 'train')\n",
    "corpus_valid, spans_valid, _, _, valid_lines = process_corpus('/content/hw3/trees/valid.txt', 'eval',\n",
    "                                                 vocab_dict=vocab_dict, label_dict=label_dict)\n",
    "corpus_test,  spans_test, _, _, test_lines = process_corpus('/content/hw3/trees/test.txt', 'eval',\n",
    "                                                 vocab_dict=vocab_dict, label_dict=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XmV9y4CMuTmC"
   },
   "outputs": [],
   "source": [
    "# inverted dictionaries {word_index: word, ...}\n",
    "inv_vocab_dict = np.array(list(vocab_dict.keys()))\n",
    "inv_label_dict = np.array(list(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GydAR_w6VhHk",
    "outputId": "ebd0ef32-26ed-4b93-9189-de68eac45dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 8626\n",
      "Number of different labels: 147\n"
     ]
    }
   ],
   "source": [
    "num_words = len(vocab_dict)\n",
    "num_labels = len(label_dict)\n",
    "\n",
    "print('Number of different words: {}'.format(num_words))\n",
    "print('Number of different labels: {}'.format(num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca3PHpoIulTv"
   },
   "source": [
    "Let see how the data looks like, and compare with our output in below:\n",
    "```\n",
    "['how', 'long', 'will', 'it', 'take', 'to', 'drive', 'from', 'chicago', 'to', 'mississippi']\n",
    "how long will it take to drive from chicago to mississippi\t[IN:GET_ESTIMATED_DURATION how [SUB long [SUB will [SUB it [SUB take [SUB to [SUB [SL:METHOD_TRAVEL drive ] [SUB from [SUB [SL:SOURCE chicago ] [SUB to [SL:DESTINATION mississippi ] ] ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "[[(0, 11), 3], [(0, 1), 1], [(1, 11), 4], [(1, 2), 1], [(2, 11), 4], [(2, 3), 1], [(3, 11), 4], [(3, 4), 1], [(4, 11), 4], [(4, 5), 1], [(5, 11), 4], [(5, 6), 1], [(6, 11), 4], [(6, 7), 5], [(7, 11), 4], [(7, 8), 1], [(8, 11), 4], [(8, 9), 6], [(9, 11), 4], [(9, 10), 1], [(10, 11), 7]]\n",
    "['will', 'it', 'take', 'shorter', 'to', 'get', 'to', 'the', 'white', 'house', 'by', 'bus', 'or', 'taxi', '?']\n",
    "will it take shorter to get to the white house by bus or taxi ?\t[IN:UNSUPPORTED_NAVIGATION will [SUB it [SUB take [SUB shorter [SUB to [SUB get [SUB to [SUB the [SUB white [SUB house [SUB by [SUB bus [SUB or [SUB taxi ? ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "[[(0, 15), 8], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 1]]\n",
    "['will', 'i', 'make', 'it', 'to', 'the', 'beach', 'by', 'noon', 'if', 'i', 'leave', 'now']\n",
    "will i make it to the beach by noon if i leave now\t[IN:GET_ESTIMATED_ARRIVAL will [SUB i [SUB make [SUB it [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:CATEGORY_LOCATION the beach ] [SUB [SL:DATE_TIME_ARRIVAL by noon ] [SUB if [SUB i [SUB leave [SL:DATE_TIME_DEPARTURE now ] ] ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "[[(0, 13), 9], [(0, 1), 1], [(1, 13), 4], [(1, 2), 1], [(2, 13), 4], [(2, 3), 1], [(3, 13), 4], [(3, 4), 1], [(4, 13), 4], [(4, 5), 1], [(5, 13), 4], [(5, 7), 10], [(5, 6), 1], [(6, 7), 1], [(7, 13), 4], [(7, 9), 11], [(7, 8), 1], [(8, 9), 1], [(9, 13), 4], [(9, 10), 1], [(10, 13), 4], [(10, 11), 1], [(11, 13), 4], [(11, 12), 1], [(12, 13), 12]]\n",
    "['when', 'should', 'i', 'leave', 'my', 'house', 'to', 'get', 'to', 'the', 'hamilton', 'mall', 'right', 'when', 'it', 'opens', 'on', 'saturday']\n",
    "when should i leave my house to get to the hamilton mall right when it opens on saturday\t[IN:GET_ESTIMATED_DEPARTURE when [SUB should [SUB i [SUB leave [SUB [SL:SOURCE--IN:GET_LOCATION_HOME [SL:CONTACT my ] house ] [SUB to [SUB get [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:POINT_ON_MAP the [SUB hamilton mall ] ] [SL:DATE_TIME_ARRIVAL right [SUB when [SUB it [SUB opens [SUB on saturday ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "[[(0, 18), 13], [(0, 1), 1], [(1, 18), 4], [(1, 2), 1], [(2, 18), 4], [(2, 3), 1], [(3, 18), 4], [(3, 4), 1], [(4, 18), 4], [(4, 6), 14], [(4, 5), 15], [(5, 6), 1], [(6, 18), 4], [(6, 7), 1], [(7, 18), 4], [(7, 8), 1], [(8, 18), 4], [(8, 9), 1], [(9, 18), 4], [(9, 12), 16], [(9, 10), 1], [(10, 12), 4], [(10, 11), 1], [(11, 12), 1], [(12, 18), 11], [(12, 13), 1], [(13, 18), 4], [(13, 14), 1], [(14, 18), 4], [(14, 15), 1], [(15, 18), 4], [(15, 16), 1], [(16, 18), 4], [(16, 17), 1], [(17, 18), 1]]\n",
    "['i', 'need', 'to', 'know', 'if', 'there', \"'s\", 'a', 'lot', 'of', 'traffic', 'on', 'my', 'way', 'home']\n",
    "i need to know if there 's a lot of traffic on my way home\t[IN:GET_INFO_TRAFFIC i [SUB need [SUB to [SUB know [SUB if [SUB there [SUB 's [SUB a [SUB lot [SUB of [SUB traffic [SUB on [SUB my [SUB way [SL:DESTINATION--IN:GET_LOCATION_HOME home ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "[[(0, 15), 17], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 18]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhF6Aw05un1X",
    "outputId": "9a58870e-461a-4191-cb4c-06f36d19c126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'long', 'will', 'it', 'take', 'to', 'drive', 'from', 'chicago', 'to', 'mississippi']\n",
      "how long will it take to drive from chicago to mississippi\t[IN:GET_ESTIMATED_DURATION how [SUB long [SUB will [SUB it [SUB take [SUB to [SUB [SL:METHOD_TRAVEL drive ] [SUB from [SUB [SL:SOURCE chicago ] [SUB to [SL:DESTINATION mississippi ] ] ] ] ] ] ] ] ] ] ]\n",
      "[[(0, 11), 3], [(0, 1), 1], [(1, 11), 4], [(1, 2), 1], [(2, 11), 4], [(2, 3), 1], [(3, 11), 4], [(3, 4), 1], [(4, 11), 4], [(4, 5), 1], [(5, 11), 4], [(5, 6), 1], [(6, 11), 4], [(6, 7), 5], [(7, 11), 4], [(7, 8), 1], [(8, 11), 4], [(8, 9), 6], [(9, 11), 4], [(9, 10), 1], [(10, 11), 7]]\n",
      "\n",
      "['will', 'it', 'take', 'shorter', 'to', 'get', 'to', 'the', 'white', 'house', 'by', 'bus', 'or', 'taxi', '?']\n",
      "will it take shorter to get to the white house by bus or taxi ?\t[IN:UNSUPPORTED_NAVIGATION will [SUB it [SUB take [SUB shorter [SUB to [SUB get [SUB to [SUB the [SUB white [SUB house [SUB by [SUB bus [SUB or [SUB taxi ? ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "[[(0, 15), 8], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 1]]\n",
      "\n",
      "['will', 'i', 'make', 'it', 'to', 'the', 'beach', 'by', 'noon', 'if', 'i', 'leave', 'now']\n",
      "will i make it to the beach by noon if i leave now\t[IN:GET_ESTIMATED_ARRIVAL will [SUB i [SUB make [SUB it [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:CATEGORY_LOCATION the beach ] [SUB [SL:DATE_TIME_ARRIVAL by noon ] [SUB if [SUB i [SUB leave [SL:DATE_TIME_DEPARTURE now ] ] ] ] ] ] ] ] ] ] ]\n",
      "[[(0, 13), 9], [(0, 1), 1], [(1, 13), 4], [(1, 2), 1], [(2, 13), 4], [(2, 3), 1], [(3, 13), 4], [(3, 4), 1], [(4, 13), 4], [(4, 5), 1], [(5, 13), 4], [(5, 7), 10], [(5, 6), 1], [(6, 7), 1], [(7, 13), 4], [(7, 9), 11], [(7, 8), 1], [(8, 9), 1], [(9, 13), 4], [(9, 10), 1], [(10, 13), 4], [(10, 11), 1], [(11, 13), 4], [(11, 12), 1], [(12, 13), 12]]\n",
      "\n",
      "['when', 'should', 'i', 'leave', 'my', 'house', 'to', 'get', 'to', 'the', 'hamilton', 'mall', 'right', 'when', 'it', 'opens', 'on', 'saturday']\n",
      "when should i leave my house to get to the hamilton mall right when it opens on saturday\t[IN:GET_ESTIMATED_DEPARTURE when [SUB should [SUB i [SUB leave [SUB [SL:SOURCE--IN:GET_LOCATION_HOME [SL:CONTACT my ] house ] [SUB to [SUB get [SUB to [SUB [SL:DESTINATION--IN:GET_LOCATION--SL:POINT_ON_MAP the [SUB hamilton mall ] ] [SL:DATE_TIME_ARRIVAL right [SUB when [SUB it [SUB opens [SUB on saturday ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "[[(0, 18), 13], [(0, 1), 1], [(1, 18), 4], [(1, 2), 1], [(2, 18), 4], [(2, 3), 1], [(3, 18), 4], [(3, 4), 1], [(4, 18), 4], [(4, 6), 14], [(4, 5), 15], [(5, 6), 1], [(6, 18), 4], [(6, 7), 1], [(7, 18), 4], [(7, 8), 1], [(8, 18), 4], [(8, 9), 1], [(9, 18), 4], [(9, 12), 16], [(9, 10), 1], [(10, 12), 4], [(10, 11), 1], [(11, 12), 1], [(12, 18), 11], [(12, 13), 1], [(13, 18), 4], [(13, 14), 1], [(14, 18), 4], [(14, 15), 1], [(15, 18), 4], [(15, 16), 1], [(16, 18), 4], [(16, 17), 1], [(17, 18), 1]]\n",
      "\n",
      "['i', 'need', 'to', 'know', 'if', 'there', \"'s\", 'a', 'lot', 'of', 'traffic', 'on', 'my', 'way', 'home']\n",
      "i need to know if there 's a lot of traffic on my way home\t[IN:GET_INFO_TRAFFIC i [SUB need [SUB to [SUB know [SUB if [SUB there [SUB 's [SUB a [SUB lot [SUB of [SUB traffic [SUB on [SUB my [SUB way [SL:DESTINATION--IN:GET_LOCATION_HOME home ] ] ] ] ] ] ] ] ] ] ] ] ] ] ]\n",
      "[[(0, 15), 17], [(0, 1), 1], [(1, 15), 4], [(1, 2), 1], [(2, 15), 4], [(2, 3), 1], [(3, 15), 4], [(3, 4), 1], [(4, 15), 4], [(4, 5), 1], [(5, 15), 4], [(5, 6), 1], [(6, 15), 4], [(6, 7), 1], [(7, 15), 4], [(7, 8), 1], [(8, 15), 4], [(8, 9), 1], [(9, 15), 4], [(9, 10), 1], [(10, 15), 4], [(10, 11), 1], [(11, 15), 4], [(11, 12), 1], [(12, 15), 4], [(12, 13), 1], [(13, 15), 4], [(13, 14), 1], [(14, 15), 18]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print([inv_vocab_dict[w] for w in corpus_train[i]])\n",
    "  print(train_lines[i], end=\"\")\n",
    "  print(spans_train[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEVW10A2uqMk"
   },
   "source": [
    "### **Defining the Neural Network**\n",
    "\n",
    "#### **Sentence Encoding**\n",
    "\n",
    "We use a Bi-directional LSTM for sentence encoding. We build a sentence encoder with a embedding layer and a Bi-directional LSTM layer:\n",
    "\n",
    "- Input: \n",
    " - word indices: `[batch_size, sentence_length]`\n",
    "- Output: \n",
    "  - word embeddings: `[batch_size, sentence_length, 2*hidden_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "y2IACimiulvO"
   },
   "outputs": [],
   "source": [
    "class SentEnc(nn.Module):\n",
    "    def __init__(self, num_words, num_layers, hidden_size, dropout=0):\n",
    "        super(SentEnc, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(num_words, hidden_size)\n",
    "        # --------- Your code --------- #\n",
    "        # Construct your lstm module here (single line):\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # --------- Your code ends --------- #\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch_size, entence_length] matrix of word indices\n",
    "        This function should return a matrix of \n",
    "        [batch_size, sentence_length, 2*hidden_size] word embeddings. \n",
    "        '''\n",
    "        # --------- Your code --------- #\n",
    "        # Hint: remember to pass the inputs through the embedding layer! \n",
    "        outputs = self.embedding(x)\n",
    "        outputs, _ = self.lstm(outputs)\n",
    "\n",
    "        # --------- Your code ends --------- #\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjR9iev7uv7d"
   },
   "source": [
    "### **Span Encodings**\n",
    "\n",
    "Given the LSTM outputs, we generate the span embeddings with the span indices.\n",
    "\n",
    "We generate a span embedding by concatenating the word embeddings of the first and last words of a span. For example, if a span starts from the i-th word and ends at the j-th word, our span embedding would be\n",
    "\n",
    "$$[h_i^T; h_{j-1}^T]^T$$\n",
    "\n",
    "where $h_i$ stands for the Bi-LSTM output of the $i^{th}$ word. Note that span_ij is inclusive to i but exclusive to j, as would be the output if you sliced a Python list A[i:j]. \n",
    "\n",
    "\n",
    "In Pytorch, Given the hidden states $h[0], h[1], ..., h[n]$, where\n",
    "```\n",
    "h[i].size() = [1, k]\n",
    "```\n",
    "the embedding of span (i, j) is\n",
    "```\n",
    "span_ij = torch.cat([h[i], h[j-1]], dim=1)\n",
    "span_ij.size() = [1, 2 * k]\n",
    "```\n",
    "Please complete the following function for generating span embeddings.\n",
    "\n",
    "- Input: \n",
    " - word embeddings: `[sentence_length, hidden_size]` \n",
    " - span indices: `[num_span, 2]`\n",
    "- Output: \n",
    " - span embeddings `[num_span, hidden_size * 2]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Cuwyy4GRuwQO"
   },
   "outputs": [],
   "source": [
    "def get_span_embeddings(word_embeddings, span_indices):\n",
    "    '''\n",
    "    word_embeddings: [sentence_length, hidden_size] matrix of each word's' embeddings from the sentence\n",
    "    span_indices: [num_span, 2] matrix of all span indices\n",
    "    '''\n",
    "    # --------- Your code --------- #\n",
    "    sentence_length, hidden_size = word_embeddings.shape\n",
    "    num_span = span_indices.size(0)\n",
    "    span_embeddings = torch.zeros((num_span, hidden_size * 2)).to(device)\n",
    "    for i in range(num_span):\n",
    "        span_begin = span_indices[i, 0]\n",
    "        span_end = span_indices[i, 1] - 1\n",
    "        span_embeddings[i] = torch.cat([word_embeddings[span_begin], word_embeddings[span_end]], dim=-1)\n",
    "    return span_embeddings\n",
    "    # --------- Your code ends --------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pAH7yGlu3oo"
   },
   "source": [
    "### **Tag Prediction**\n",
    "\n",
    "We build a Classifier that puts the neural models together. The classifier takes word and span indices as inputs, and predict span labels by calculating word embeddings, span embeddings, and label logits. we will predict the tag of the spans with a linear classifier.\n",
    "\n",
    "- Inputs: \n",
    " - word indices: `[batch_size, num_words]`\n",
    "- Outputs: \n",
    " - span predictions: `[num_spans, num_labels]`\n",
    "\n",
    "Please implement the forward function following 4 steps:\n",
    "1. Generate the word embeddings by processing the input sentences with the LSTM sentence encoder.\n",
    "2. Apply dropout on word embeddings.\n",
    "3. Calculate span embeddings with function get_span_embeddings().\n",
    "4. Calculate label logits with the linear layer defined as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hj-JQKiJu6hM"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_words, num_labels, num_layers, hidden_size, dropout=0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.sent_enc = SentEnc(num_words, num_layers, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(4 * hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, x, span_indices):\n",
    "      '''\n",
    "      x: [batch_size, num_words] matrix of the word indices\n",
    "      span_indices: [num_spans, 2] matrix of the span indices\n",
    "      This function should return a matrix of [num_spans, num_labels] where\n",
    "      each row contains the label logits corresponding to the relevant span. \n",
    "      You can assume batch_size is 1.\n",
    "      '''\n",
    "      # --------- Your code --------- #\n",
    "      x = self.sent_enc(x)\n",
    "      x = self.dropout(x)\n",
    "      span_embeddings = get_span_embeddings(x[0], span_indices)\n",
    "      logits = self.linear(span_embeddings)\n",
    "      \n",
    "      # --------- Your code ends --------- #\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nGHxCGPzu8HR"
   },
   "outputs": [],
   "source": [
    "#For decoding, we add some random spans and label them as \"None\"\n",
    "def add_none_span(word_list, span_list, label_dict, all=False):\n",
    "    num_words = len(word_list)\n",
    "    num_labeled_span = len(span_list)\n",
    "    labeled_span_set = set([span for span, label in span_list])\n",
    "    none_spans = []\n",
    "    for i in range(num_words):\n",
    "        for j in range(i + 1, num_words):\n",
    "            if (i, j) not in labeled_span_set:\n",
    "                none_spans.append([(i, j), label_dict['None']])\n",
    "    if not all:\n",
    "        k = min(num_labeled_span, len(none_spans))\n",
    "        sampled_none_spans = random.sample(none_spans, k)\n",
    "    else:\n",
    "        sampled_none_spans = none_spans\n",
    "    return span_list + sampled_none_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6MFlda6u97Y"
   },
   "source": [
    "### **Training Loop**\n",
    "\n",
    "With all neural models already defined, we can now implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA8ZmNTzu2ww",
    "outputId": "68a81bd1-a282-4642-941f-10d44b885243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Classifier(\n",
      "  (sent_enc): SentEnc(\n",
      "    (embedding): Embedding(8626, 200)\n",
      "    (lstm): LSTM(200, 200, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (linear): Linear(in_features=800, out_features=147, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/31279 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10020/31279 [01:47<03:38, 97.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 20012/31279 [03:35<02:07, 88.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 30020/31279 [05:24<00:12, 97.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31279/31279 [05:37<00:00, 92.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss=0.19886971725613117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4462/4462 [00:20<00:00, 220.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, valid loss=0.10116364356324448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/31279 [00:00<07:04, 73.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10012/31279 [01:49<03:47, 93.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 20014/31279 [03:36<02:10, 86.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 30019/31279 [05:24<00:13, 96.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31279/31279 [05:37<00:00, 92.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss=0.09100156822784086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4462/4462 [00:21<00:00, 209.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, valid loss=0.08298133122400654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/31279 [00:00<07:25, 70.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10013/31279 [01:47<03:35, 98.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 20015/31279 [03:35<02:07, 88.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 30015/31279 [05:22<00:14, 89.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31279/31279 [05:36<00:00, 92.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss=0.06274110535926637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4462/4462 [00:20<00:00, 222.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, valid loss=0.08049911542311732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Using device: {}'.format(device))\n",
    "\n",
    "\n",
    "# just remember you can tune these hyper-parameters!\n",
    "batch_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 200\n",
    "lr = 0.05\n",
    "num_epochs = 3 # Be aware of over-fitting!\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "dropout = 0.25\n",
    "\n",
    "classifier = Classifier(num_words, num_labels, num_layers, hidden_size, dropout)\n",
    "print(classifier)\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "classifier = classifier.to(device)\n",
    "classifier.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    classifier.train()\n",
    "    for i in tqdm(range(len(corpus_train))):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print('Epoch {} Batch {}'.format(epoch, i))\n",
    "        \n",
    "        cur_spans = add_none_span(corpus_train[i], spans_train[i], label_dict)\n",
    "        \n",
    "        sent_inputs  = torch.Tensor([corpus_train[i]]).long().to(device)\n",
    "        span_indices = torch.Tensor([x[0] for x in cur_spans]).long().to(device)\n",
    "        span_labels  = torch.Tensor([x[1] for x in cur_spans]).long().to(device)\n",
    "        \n",
    "          # This should follow the same training process from past homeworks. Don't\n",
    "        # forget to increment your total_loss! \n",
    "        # --------- Your code --------- #\n",
    "\n",
    "        preds = classifier(sent_inputs, span_indices)\n",
    "\n",
    "        loss = loss_fn(preds, span_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "        # --------- Your code ends --------- #\n",
    "    print('Epoch {}, train loss={}'.format(epoch, total_loss / len(corpus_train)))\n",
    "\n",
    "    total_loss = 0\n",
    "    classifier.eval()\n",
    "    for i in tqdm(range(len(corpus_valid))):\n",
    "        #if i % 10000 == 0:\n",
    "        #    print('Epoch {} Batch {}'.format(epoch, i))\n",
    "        cur_spans = add_none_span(corpus_valid[i], spans_valid[i], label_dict)\n",
    "        \n",
    "        sent_inputs  = torch.Tensor([corpus_valid[i]]).long().to(device)\n",
    "        span_indices = torch.Tensor([x[0] for x in cur_spans]).long().to(device)\n",
    "        span_labels  = torch.Tensor([x[1] for x in cur_spans]).long().to(device)\n",
    "        \n",
    "        # This is very similar to the training steps above, but you won't need to\n",
    "        # back-propogate and update weights during validation.\n",
    "        # --------- Your code --------- #\n",
    "\n",
    "        preds = classifier(sent_inputs, span_indices)\n",
    "        loss = loss_fn(preds, span_labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # --------- Your code ends --------- #\n",
    "    print('Epoch {}, valid loss={}'.format(epoch, total_loss / len(corpus_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6oZtDWRvDWs"
   },
   "source": [
    "### **Evaluation**\n",
    "\n",
    "After training the model, we evaluate the classification results.  \n",
    "What we will do is that we treat a tree strcture as a bag of spans (a list of span indices), and then compute F-1 score.  \n",
    "The staff solution computed precision near 0.85, recall near 0.94, f1 near 0.89, and exact_match near 0.42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "spzm1vmUvCVY"
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from typing import Counter, Dict, Optional\n",
    "import numpy as np\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self, strict = False) -> None:\n",
    "        self.TP = 0\n",
    "        self.gold_P = 0\n",
    "        self.pred_P = 0      \n",
    "        self.exact_match = []\n",
    "        self.tree_match = []\n",
    "        self.well_form = []\n",
    "        self.strict = strict\n",
    "\n",
    "    def get_metrics(self):\n",
    "        precision = (self.TP / self.pred_P) if self.pred_P else 0\n",
    "        recall = (self.TP / self.gold_P) if self.gold_P else 0\n",
    "        f1 = (2.0 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
    "     \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"exact_match\": np.mean(self.exact_match),\n",
    "            \"well_form\": np.mean(self.well_form),\n",
    "            \"tree_match\":  np.mean(self.tree_match),\n",
    "            \"num_examples\": len(self.exact_match)\n",
    "        }\n",
    "    \n",
    "    def is_well_formed(self, spans):   \n",
    "        for s1 in spans: \n",
    "          for s2 in spans:\n",
    "              if s1[0] < s2[0] and s2[0] < s1[1] and s1[1] < s2[1]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def add_instance_span(self, gold_spans, pred_spans):\n",
    "        self.gold_P += len(gold_spans)\n",
    "        self.pred_P += len(pred_spans)\n",
    "        self.TP += len(set(gold_spans) & set(pred_spans))\n",
    "        self.exact_match.append(int(set(gold_spans) == set(pred_spans)))\n",
    "        gold_spans = [s[0] for s in gold_spans]\n",
    "        pred_spans = [s[0] for s in pred_spans]\n",
    "        self.tree_match.append(int(set(gold_spans) == set(pred_spans)))\n",
    "        well_formed = self.is_well_formed(pred_spans)\n",
    "        self.well_form.append(int(well_formed))\n",
    "\n",
    "    def add_instance_tree(self, gold_tree, pred_tree):\n",
    "        node_info_gold = self._get_node_info(gold_tree)\n",
    "        self.gold_P += len(node_info_gold)\n",
    "        node_info_pred = self._get_node_info(pred_tree)\n",
    "        self.pred_P += len(node_info_pred)\n",
    "        self.TP += len(node_info_gold & node_info_pred)\n",
    "        self.exact_match.append(int(node_info_gold.keys() == node_info_pred.keys()))\n",
    "        self.well_form.append(1) #we assume the decoded tree is indeed a tree :)\n",
    "        node_info_gold = {k[1] for k,v in node_info_gold.items()}\n",
    "        node_info_pred = {k[1] for k,v in node_info_pred.items()}\n",
    "        self.tree_match.append(int(node_info_gold==node_info_pred))\n",
    "        \n",
    "    def _get_node_info(self, tree) -> Counter:\n",
    "        nodes = tree.root.list_nonterminals()\n",
    "        node_info: Counter = Counter()\n",
    "        for node in nodes:\n",
    "            if node.label != 'Token':\n",
    "              span = self._get_span(node)\n",
    "              node_info[(node.label, self._get_span(node))] += 1 \n",
    "\n",
    "        return node_info\n",
    "\n",
    "    def _get_span(self, node):\n",
    "        return node.get_flat_str_spans(\n",
    "        ) if self.strict else node.get_token_span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CWKgLdPvH9Q",
    "outputId": "1503d93e-e794-4248-a33f-6247cbbfe1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8490942277460672, 'recall': 0.9437024355351259, 'f1': 0.8939020326557814, 'exact_match': 0.41647215738579524, 'well_form': 0.5489607646993442, 'tree_match': 0.48338335000555743, 'num_examples': 8997}\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "partb_calc = Calculator(strict=False)\n",
    "pred_bag_spans = []\n",
    "gold_bag_spans = []\n",
    "for (tokens, spans, line) in zip(corpus_test,spans_test,test_lines):   \n",
    "    #We only test non-Token labels\n",
    "    spans = [tuple(x) for x in spans if x[1] != 1]\n",
    "\n",
    "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
    "      continue\n",
    "\n",
    "    all_spans = [(i,j) for i in range(len(tokens)) \n",
    "                        for j in range(i + 1, len(tokens) + 1)]\n",
    "\n",
    "    input  = torch.Tensor([tokens]).long().to(device)\n",
    "    logits = classifier(input, torch.Tensor(all_spans).long().to(device))\n",
    "\n",
    "    pred_spans = []\n",
    "    for i, span in enumerate(all_spans):\n",
    "        label_idx = torch.argmax(logits[i]).item()\n",
    "        if label_idx != 2 and label_idx != 1:\n",
    "          pred_spans.append((span,label_idx))\n",
    "    \n",
    "    partb_calc.add_instance_span(spans, pred_spans)\n",
    "    pred_bag_spans.append(pred_spans)\n",
    "    gold_bag_spans.append(spans)\n",
    " \n",
    "print(partb_calc.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mh8C9Q_vMLd"
   },
   "source": [
    "## **PART C** (Only for 6.864 students)\n",
    "The remaining will be **Part C** for **HW3-Trees**.  \n",
    "In Part C, we will decode a tree based on the classifier trained on Part B.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ-gP9AqnSc2"
   },
   "source": [
    "### **CKY**  \n",
    "You will be implementing the following simple CYK recursion:  \n",
    "```best_score[i,j]=max_k {best_score[i,k]+best_score[k,j]} + max_l {span_dict[(i,j)][l]}```      \n",
    "where `l` is the label of the current span `(i,j)`, and `k` is the splitting point. `k` is inclusive to the right span. \n",
    "\n",
    "Note that this is a simpler recursion than the full CKY algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEdipYPwvMlG"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "EPS = 1e-6\n",
    "dp_results = []\n",
    "classifier.eval()\n",
    "for kk,(line,spans,tokens) in enumerate(zip(test_lines,spans_test,corpus_test)):\n",
    "    spans = [tuple(x) for x in spans if  x[1] != 1]\n",
    "    \n",
    "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
    "      continue\n",
    "    \n",
    "    sent_inputs  = torch.Tensor([tokens]).long().to(device)\n",
    "    \n",
    "    all_spans = [(i,j) for i in range(len(tokens)) \n",
    "                         for j in range(i + 1, len(tokens) + 1)]\n",
    "    \n",
    "    logits = classifier(sent_inputs, torch.Tensor(all_spans).long().to(device))\n",
    "    logprobs = log_softmax(logits, dim = -1)\n",
    "    # span dict will map each span (l,r) to its predicted distribution of labels\n",
    "    span_dict = {}\n",
    "    for i, s in enumerate(all_spans): \n",
    "      span_dict[s]  = logprobs[i] \n",
    "  \n",
    "    TOKEN_ID, NULL_ID = 1, 2\n",
    "    best_score, best_split, best_label = {}, {}, {} # we will do dynamic programming to decode a binary tree out of our predictions\n",
    "    # Think: why do we first iterate the length of the span?\n",
    "    for ll in range(1, len(tokens) + 1): # length of the span\n",
    "        for i in range(0, len(tokens)-ll+1): # start of the span\n",
    "            j = i + ll\n",
    "            cur_span = (i, j)\n",
    "            if j == i + 1:\n",
    "                span_dict[cur_span][NULL_ID]  = -1/EPS\n",
    "                # --------- Your code --------- #\n",
    "                #use span_dict[cur_span] to update best_label and best_score              \n",
    "\n",
    "                \n",
    "                # --------- Your code ends --------- #\n",
    "                best_split[cur_span] = None\n",
    "            else:\n",
    "                span_dict[cur_span][NULL_ID]  = -1/EPS # we will never decode a NULL sub-tree\n",
    "                span_dict[cur_span][TOKEN_ID] = -1/EPS # we will never decode a NULL sub-tree\n",
    "                # --------- Your code --------- #\n",
    "                #try to give the values for best_score/label/split[cur_span] using the \n",
    "                # recursive equation above\n",
    "\n",
    "\n",
    "\n",
    "                # --------- Your code ends --------- #\n",
    "            #print(cur_span, best_score[cur_span], best_label[cur_span])\n",
    "    dp_results.append((best_score, best_split, best_label))\n",
    "print(len(dp_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_HmzvWevUIa"
   },
   "source": [
    "### **Tree Construction**\n",
    "In this section, we will construct a tree using the DP results.  \n",
    "The code relies on a `Node` class defined in `span_tree.py`. Each `Node` object has `children`, `parent`, and `label` attributes, where `children` is a `List`, `parent` is an optional `Node`, and `label` is a `str`. \n",
    "\n",
    "You won't need more than this to fill out the code, but feel free to look more into `span_tree.py` for a better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAF9XsuUvQYK"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QClkqbLKvZRF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def get_nodetype(label):\n",
    "    if label.startswith(PREFIX_INTENT):\n",
    "        node = Intent(label)\n",
    "    elif label.startswith(PREFIX_SLOT):\n",
    "        node = Slot(label)\n",
    "    elif label.startswith(PREFIX_SUBTREE):\n",
    "        node = SubTree(label)\n",
    "    else:\n",
    "        print('something wrong with the label!!!', label)\n",
    "        sys.error()\n",
    "    return node\n",
    "\n",
    "def dfs_build(l, r, best_label, best_split):\n",
    "  '''\n",
    "  l: integer representing left (inclusive) index of span\n",
    "  r: integer representing right (non-inclusive) index of span\n",
    "  best_label: {span: label_index} dictionary created in dp_results\n",
    "  best_split: {span: split} dictionary created in dp_results\n",
    "  This function returns the node for the given span, recursively \n",
    "  creating all children nodes below it. \n",
    "  '''\n",
    "    if l + 1 == r:\n",
    "        la = best_label[(l,r)]\n",
    "        if la == 1:\n",
    "            return Token(surface_tokens[l], l)\n",
    "        else:\n",
    "            node = get_nodetype(inv_label_dict[la])\n",
    "            node.children = [Token(surface_tokens[l], l)]\n",
    "            node.children[0].parent = node\n",
    "            return node\n",
    "\n",
    "    label = inv_label_dict[best_label[(l, r)]]\n",
    "    node = get_nodetype(label)\n",
    "    \n",
    "    #--- your code --- #\n",
    "    #hint: use best_split! and recursion to assign node.children here\n",
    "\n",
    "\n",
    "    #--- your code ends --- #\n",
    "\n",
    "    for c in node.children:\n",
    "        c.parent = node\n",
    "    \n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knPtnKzBvZ2K"
   },
   "outputs": [],
   "source": [
    "pred_trees = []\n",
    "gold_trees = []\n",
    "partc_calc = Calculator(strict=False)\n",
    "k = 0\n",
    "for i,(line,spans,tokens) in enumerate(zip(test_lines,spans_test,corpus_test)):\n",
    "    surface_tokens, str_ref_tree = line.strip().split('\\t')\n",
    "    surface_tokens = surface_tokens.split()\n",
    "    spans = [tuple(x) for x in spans if x[1] != 1]\n",
    "\n",
    "    if len(spans) <= 1 or len(line.strip()) < 3: \n",
    "      continue\n",
    "\n",
    "    best_score, best_split, best_label = dp_results[k]\n",
    "    k+=1\n",
    "    root = Root()\n",
    "    root.children = [dfs_build(0, len(tokens), best_label, best_split)]\n",
    "    root.children[0].parent = root\n",
    "    tree = Tree('IN:GET_EVENT placeholder') #the string here is just a placeholder\n",
    "    tree.root = root\n",
    "    if k < 10: #use this info for debugging! Does your tree make sense?\n",
    "        print(k, line.strip())\n",
    "        print('REF:', str_ref_tree)\n",
    "        print('DEC:', str(tree))\n",
    "        print()\n",
    "    \"\"\" here's some decoding examples we get\n",
    "      1 whats there to do this weekend\t[IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
    "      REF: [IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
    "      DEC: [IN:GET_EVENT whats [SUB there [SUB to [SUB do [SL:DATE_TIME this weekend ] ] ] ] ]\n",
    "\n",
    "      2 what is a good restaurant for tex mex in austin\t[IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
    "      REF: [IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
    "      DEC: [IN:UNSUPPORTED what [SUB is [SUB a [SUB good [SUB restaurant [SUB for [SUB tex [SUB mex [SUB in austin ] ] ] ] ] ] ] ] ]\n",
    "\n",
    "      3 where can i see the fireworks tonight\t[IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB [SL:CATEGORY_EVENT the fireworks ] [SL:DATE_TIME tonight ] ] ] ] ] ]\n",
    "      REF: [IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB [SL:CATEGORY_EVENT the fireworks ] [SL:DATE_TIME tonight ] ] ] ] ] ]\n",
    "      DEC: [IN:GET_EVENT where [SUB can [SUB i [SUB see [SUB the [SUB fireworks [SL:DATE_TIME tonight ] ] ] ] ] ] ]\n",
    "    \"\"\"\n",
    "    partc_calc.add_instance_tree(Tree(str_ref_tree), tree)\n",
    "    pred_trees.append(tree)\n",
    "    gold_trees.append(Tree(str_ref_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je1Mwb0YveZs"
   },
   "source": [
    "Once again, we'll look at the metrics. \n",
    "The staff solution computed precision near 0.87, recall near 0.86, f1 near 0.87, and exact_match near 0.44. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otiwx7ZLvdye"
   },
   "outputs": [],
   "source": [
    "print(partc_calc.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6IXH-E9vgx6"
   },
   "source": [
    "\n",
    "Recommended Reading (not required, just for interested students):  \n",
    "https://arxiv.org/pdf/1810.07942.pdf  \n",
    "https://www.aclweb.org/anthology/D16-1257/  \n",
    "https://arxiv.org/abs/1412.7449  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAFVYD-Hviet"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
