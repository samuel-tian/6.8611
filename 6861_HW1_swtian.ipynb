{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU7xWiY6TyWS",
        "outputId": "01e48c32-7646-4e69-bf9e-2a7fc3fdcd37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'hw2'...\n"
          ]
        }
      ],
      "source": [
        "# %%bash\n",
        "# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "# rm -rf hw2\n",
        "# git clone https://github.com/mit-6864/hw2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A0MHaHrdUACZ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"hw2\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lab_util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ3MUj4iUf76"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, you will find code scaffolding for the implementation portion of Homework 1. There are certain parts of the scaffolding marked with `# Your code here!` comments where you can fill in code to perform the specified tasks. You should be able to complete this assignment without changing any of the scaffolding code, just writing code to fill in the scaffolding and run experiments. Make sure to read the text between cells for certain implementation details. Please submit the notebook with all code cells run.\n",
        "\n",
        "This notebook can be done independently of the handout and will be graded based on the code and cell outputs. However, certain questions on the handout require you to design and perform experiments to evaluate the methods used here. There is space at the end of the notebook for you to carry these experiments out. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG654Y9J3yHw"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We're going to be working with a dataset of product reviews. The following cell loads the dataset and splits it into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JwiX-Tc9V1xI",
        "outputId": "814d4c3d-7396-4075-aa18-830165701286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "rating: 0 (bad)\n",
            "\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "rating: 1 (good)\n",
            "\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "rating: 1 (good)\n",
            "\n",
            "Read 4000 total reviews.\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"hw2/reviews.csv\") as reader:\n",
        "    csvreader = csv.reader(reader)\n",
        "    next(csvreader)\n",
        "    for id, review, label in csvreader:\n",
        "        label = int(label)\n",
        "\n",
        "        # hacky class balancing\n",
        "        if label == 1:\n",
        "            if n_positive == 2000:\n",
        "                continue\n",
        "            n_positive += 1\n",
        "        if len(data) == 4000:\n",
        "            break\n",
        "\n",
        "        data.append((review, label))\n",
        "\n",
        "        if n_disp > 5:\n",
        "            continue\n",
        "        n_disp += 1\n",
        "        print(\"review:\", review)\n",
        "        print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "        print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews, train_labels = reviews[:3000], labels[:3000]\n",
        "val_reviews, val_labels = reviews[3000:3500], labels[3000:3500]\n",
        "test_reviews, test_labels = reviews[3500:], labels[3500:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9mHkiHil1N"
      },
      "source": [
        "# Preliminaries: Word-document representations\n",
        "\n",
        "We start by constructing the bag-of-words matrix (look at `/content/hw2/lab_util.py` in the file browser on the left if you want to see how this works)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WPt6Y7-Z_7P",
        "outputId": "346feb8e-7d6a-41e7-a1a4-1dde1f38f4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW matrix is 3000 x 2006\n"
          ]
        }
      ],
      "source": [
        "vectorizer = lab_util.CountVectorizer()\n",
        "vectorizer.fit(train_reviews)\n",
        "bow_matrix = vectorizer.transform(train_reviews)\n",
        "print(f\"BoW matrix is {bow_matrix.shape[0]} x {bow_matrix.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QWx2_CJ2AT-"
      },
      "source": [
        "In class, we've seen that we can get more informative representations by using representations other than raw counts. Implement the TF-IDF transform below.\n",
        "\n",
        "Note: In lecture, we multiplied the raw term frequencies by idfs to get the TF-IDF matrix (tfidf=tf*idf). Feel free to experiment with other transformations, such as log(1+tf) for the measure of term frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1y3PmW-IgpqA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF matrix is 2006 x 3000\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class TfidfFeaturizer:\n",
        "    def fit(self, matrix):\n",
        "        # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n",
        "        # vocabulary size and `|D|` is the number of documents in the corpus. \n",
        "        # This function should create the inverse document frequency (idf) matrix\n",
        "        # for the given term-document matrix.\n",
        "\n",
        "        V, D = matrix.shape\n",
        "        normalized = matrix > 0\n",
        "        counts = np.sum(normalized, axis=-1, keepdims=True)\n",
        "        self.idf = np.log(D / counts)\n",
        "    \n",
        "    def transform_tfidf(self, matrix):\n",
        "        # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n",
        "        # vocabulary size and `|D|` is the number of documents in the corpus. This\n",
        "        # function should (nondestructively) return a version of `matrix` with the\n",
        "        # TF-IDF transform applied.\n",
        "        \n",
        "        # Your code here!\n",
        "        return matrix * self.idf\n",
        "\n",
        "td_matrix = bow_matrix.T\n",
        "featurizer = TfidfFeaturizer()\n",
        "featurizer.fit(td_matrix)\n",
        "tfidf_matrix = featurizer.transform_tfidf(td_matrix)\n",
        "print(f\"TF-IDF matrix is {tfidf_matrix.shape[0]} x {tfidf_matrix.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfIOUYjEZjw"
      },
      "source": [
        "#### Sanity check 1\n",
        "The following cell should print `True` if your `transform_tfidf` function is implemented properly. (*Hint: in our implementation, we use the natural logarithm (base $e$) when computing inverse document frequency.*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KVtphNeDEj2W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "DEBUG_sc1_matrix = np.array([[3,1,0,3,0],\n",
        "                             [0,2,0,0,1],\n",
        "                             [7,8,2,0,1],\n",
        "                             [1,9,8,1,0]])\n",
        "DEBUG_gt = np.array([[1.53247687, 0.51082562, 0.        , 1.53247687, 0.        ],\n",
        "                     [0.        , 1.83258146, 0.        , 0.        , 0.91629073],\n",
        "                     [1.56200486, 1.78514841, 0.4462871 , 0.        , 0.22314355],\n",
        "                     [0.22314355, 2.00829196, 1.78514841, 0.22314355, 0.        ]])\n",
        "debug = TfidfFeaturizer()\n",
        "debug.fit(DEBUG_sc1_matrix)\n",
        "print(np.allclose(debug.transform_tfidf(DEBUG_sc1_matrix), DEBUG_gt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1RKNTu-voxT"
      },
      "source": [
        "#### Linear models on BoW and TFIDF features\n",
        "\n",
        "Now we have two feature representations, BoW and TF-IDF. Let's first see how effective these features are for the sentiment classification task. \n",
        "\n",
        "Below, implement two logistic regression models to classify the reviews, using BoW and TF-IDF respectively. You should feel free to use the `scikit-learn` library, which has the `sklearn.linear_model.LogisticRegression` available for you. Report the training and test accuracy of these two models. \n",
        "\n",
        "Note: For the TF-IDF classifier, we only fit the IDF matrix to the training data. (Think about why you might not want a separate IDF for the test set!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "VWcucgqlzGwu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression with bag of word features\n",
            "train score: 0.9853333333333333\n",
            "test score: 0.818\n",
            "Logistic regression with tf-idf features\n",
            "train score: 1.0\n",
            "test score: 0.818\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_and_eval(train_X, train_y, test_X, test_y):\n",
        "    # Create and train a model that takes as input a feature \n",
        "    # representation of the training data and outputs a sentiment label.\n",
        "    # Make sure to report the training and test accuracy of your model.\n",
        "    model = LogisticRegression(random_state=0, max_iter=1000)\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    train_score = model.score(train_X, train_y)\n",
        "    test_score = model.score(test_X, test_y)\n",
        "    \n",
        "    return model, train_score, test_score\n",
        "    \n",
        "\n",
        "print('Logistic regression with bag of word features')\n",
        "train_bow = vectorizer.transform(train_reviews)\n",
        "test_bow = vectorizer.transform(test_reviews)\n",
        "model, train_score, test_score = train_and_eval(train_bow, train_labels, test_bow, test_labels)\n",
        "print('train score: {}'.format(train_score))\n",
        "print('test score: {}'.format(test_score))\n",
        "\n",
        "print('Logistic regression with tf-idf features')\n",
        "train_idf = featurizer.transform_tfidf(train_bow.T).T\n",
        "test_idf = featurizer.transform_tfidf(test_bow.T).T\n",
        "model_idf, train_score_idf, test_score_idf = train_and_eval(train_idf, train_labels, test_idf, test_labels)\n",
        "print('train score: {}'.format(train_score_idf))\n",
        "print('test score: {}'.format(test_score_idf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXbP130wDNwd"
      },
      "source": [
        "Let's look at what the model learns about sentiment. For both models, display the top 5 most positive and negative weights, as well as their corresponding words.\n",
        "\n",
        "Hint: look at `/content/hw2/lab_util.py` for how to convert between token indices and words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Jn89QR_3DMM_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens for BOW:\n",
            "most negative: ['stick disappointed unfortunately gf yuck']\n",
            "most positive: ['long thanks best great delicious']\n",
            "tokens for IDF\n",
            "most negative: ['not unfortunately disappointed disappointing gf']\n",
            "most positive: ['long delicious tassimo best great']\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "coefs = copy.deepcopy(model.coef_)[0].tolist()\n",
        "coefs = list(zip(coefs, list(range(len(coefs)))))\n",
        "coefs.sort()\n",
        "\n",
        "smallest = [i[1] for i in coefs[:5]]\n",
        "largest = [i[1] for i in coefs[-5:]]\n",
        "\n",
        "print(\"tokens for BOW:\")\n",
        "print(\"most negative: {}\".format(vectorizer.tokenizer.de_tokenize([smallest])))\n",
        "print(\"most positive: {}\".format(vectorizer.tokenizer.de_tokenize([largest])))\n",
        "\n",
        "coefs_idf = copy.deepcopy(model_idf.coef_)[0].tolist()\n",
        "coefs_idf = list(zip(coefs_idf, list(range(len(coefs_idf)))))\n",
        "coefs_idf.sort()\n",
        "\n",
        "smallest_idf = [i[1] for i in coefs_idf[:5]]\n",
        "largest_idf = [i[1] for i in coefs_idf[-5:]]\n",
        "\n",
        "print(\"tokens for IDF\")\n",
        "print(\"most negative: {}\".format(vectorizer.tokenizer.de_tokenize([smallest_idf])))\n",
        "print(\"most positive: {}\".format(vectorizer.tokenizer.de_tokenize([largest_idf])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twLHWqM6Z5xD"
      },
      "source": [
        "# LSA: Word representations via matrix factorization\n",
        "\n",
        "In class, we've seen that the above approaches can lead to high dimensional representations. To alleviate this, we can use latent semantic analysis (LSA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd3-pw4XbD4B"
      },
      "source": [
        "First, implement the function `learn_reps_lsa` that computes word representations via latent semantic analysis. The `sklearn.decomposition` or `np.linalg` packages may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "KASVs8KubeBE"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def learn_reps_lsa(matrix, rep_size):\n",
        "    # `matrix` is a `|V| x |D|` matrix, where `|V|` is the number of words in the\n",
        "    # vocabulary and |D| is the number of training reviews. This function should \n",
        "    # return a `|V| x rep_size` matrix with each row corresponding to a word \n",
        "    # representation. The `sklearn.decomposition` package may be useful.\n",
        "\n",
        "    # Your code here!\n",
        "    u, s, vh = np.linalg.svd(matrix)\n",
        "    return u[:, :rep_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fxv02wO9LbS"
      },
      "source": [
        "#### Sanity check 2\n",
        "The following cell contains a simple sanity check for your `learn_reps_lsa` implementation: it should print `True` if your `learn_reps_lsa` function is implemented equivalently to one of our solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "KsRlKB9q9Js-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "DEBUG_sc2_matrix = np.array([[1,0,0,2,1,3,5],\n",
        "                             [2,0,0,0,0,4,0],\n",
        "                             [0,3,4,1,8,6,6],\n",
        "                             [1,4,5,0,0,0,0]])\n",
        "\n",
        "DEBUG_reps = learn_reps_lsa(DEBUG_sc2_matrix, 3)\n",
        "DEBUG_gt1 = np.array([[ -4.92017554,  -2.85465774,   1.18575453],\n",
        "                      [ -2.14977584,  -1.19987977,   3.37221899],\n",
        "                      [-12.62664695,   0.10890093,  -1.32131745],\n",
        "                      [ -2.69216011,   5.66453534,   1.33728063]])\n",
        "DEBUG_gt2 = np.array([[-0.35188159, -0.44213061,  0.29358929],\n",
        "                      [-0.15374788, -0.18583789,  0.83495136],\n",
        "                      [-0.90303377,  0.01686662, -0.32715426],\n",
        "                      [-0.19253817,  0.87732566,  0.3311067 ]])\n",
        "\n",
        "print(np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt1)) or np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWzRC0dclVK"
      },
      "source": [
        "Let's look at some representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "-Ad7RZkwceWw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "good 47\n",
            "  gerber 1.873\n",
            "  luck 1.885\n",
            "  crazy 1.890\n",
            "  flaxseed 1.906\n",
            "  suspect 1.907\n",
            "bad 201\n",
            "  disgusting 1.625\n",
            "  horrible 1.776\n",
            "  shortbread 1.778\n",
            "  gone 1.778\n",
            "  dont 1.802\n",
            "cookie 504\n",
            "  nana's 0.964\n",
            "  bars 1.363\n",
            "  odd 1.402\n",
            "  impossible 1.459\n",
            "  cookies 1.484\n",
            "jelly 351\n",
            "  twist 1.099\n",
            "  cardboard 1.197\n",
            "  peanuts 1.311\n",
            "  advertised 1.331\n",
            "  plastic 1.510\n",
            "dog 925\n",
            "  happier 1.670\n",
            "  earlier 1.681\n",
            "  eats 1.702\n",
            "  stays 1.722\n",
            "  standard 1.727\n",
            "the 36\n",
            "  suspect 1.953\n",
            "  flowers 1.961\n",
            "  leaked 1.966\n",
            "  m 1.966\n",
            "  burn 1.967\n",
            "4 292\n",
            "  shortbread 1.674\n",
            "  toast 1.683\n",
            "  mistake 1.690\n",
            "  2nd 1.701\n",
            "  icing 1.723\n"
          ]
        }
      ],
      "source": [
        "# LSA reps for term-document matrix\n",
        "# Feel free to change the rep size!\n",
        "reps = learn_reps_lsa(td_matrix, 500)\n",
        "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"4\"]\n",
        "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obvh2vusBGpJ"
      },
      "source": [
        "How do the given similar words change if we apply LSA to the TF-IDF matrix instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "SV5xKLYTi7LA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "good 47\n",
            "  crazy 1.695\n",
            "  gerber 1.753\n",
            "  beat 1.758\n",
            "  homemade 1.785\n",
            "  tasting 1.799\n",
            "bad 201\n",
            "  disgusting 1.623\n",
            "  awful 1.713\n",
            "  positive 1.715\n",
            "  bland 1.731\n",
            "  gone 1.736\n",
            "cookie 504\n",
            "  nana's 1.103\n",
            "  moist 1.388\n",
            "  odd 1.452\n",
            "  impossible 1.486\n",
            "  needs 1.509\n",
            "jelly 351\n",
            "  twist 1.156\n",
            "  cardboard 1.211\n",
            "  advertised 1.402\n",
            "  plum 1.447\n",
            "  sold 1.470\n",
            "dog 925\n",
            "  happier 1.641\n",
            "  earlier 1.658\n",
            "  foods 1.690\n",
            "  stays 1.697\n",
            "  eats 1.704\n",
            "the 36\n",
            "  <unk> 1.478\n",
            "  and 1.578\n",
            "  . 1.581\n",
            "  of 1.627\n",
            "  is 1.632\n",
            "4 292\n",
            "  mistake 1.687\n",
            "  2nd 1.707\n",
            "  toast 1.708\n",
            "  table 1.714\n",
            "  70 1.723\n"
          ]
        }
      ],
      "source": [
        "# Feel free to change the rep size!\n",
        "reps_tfidf = learn_reps_lsa(tfidf_matrix, 500)\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO-NG4u1kG9z"
      },
      "source": [
        "Now that we have some representations, let's see if we can do something useful with them.\n",
        "\n",
        "Below, implement a feature function that represents a document as the sum of its\n",
        "learned word embeddings.\n",
        "\n",
        "The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews improve classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "6B08xvIFlee3"
      },
      "outputs": [],
      "source": [
        "import sklearn.linear_model\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def word_featurizer(xs):\n",
        "    # normalize\n",
        "    return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def lsa_featurizer(xs):\n",
        "    # This function takes in a matrix in which each row contains the word counts\n",
        "    # for the given review. It should return a matrix in which each row contains\n",
        "    # the learned feature representation of each review (e.g. the sum of LSA \n",
        "    # word representations).\n",
        "\n",
        "    feats = xs @ reps_tfidf # Your code here!\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def combo_featurizer(xs):\n",
        "    return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n",
        "\n",
        "def train_model(featurizer, xs, ys):\n",
        "    xs_featurized = featurizer(xs)\n",
        "    model = sklearn.linear_model.LogisticRegression(penalty='none', max_iter=1000)\n",
        "    model.fit(xs_featurized, ys)\n",
        "    return model\n",
        "\n",
        "def eval_model(model, featurizer, xs, ys):\n",
        "    xs_featurized = featurizer(xs)\n",
        "    pred_ys = model.predict(xs_featurized)\n",
        "    return np.mean(pred_ys == ys)\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = vectorizer.transform(train_reviews[:n_train])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = vectorizer.transform(test_reviews)\n",
        "    test_ys = test_labels\n",
        "    model = train_model(featurizer, train_xs, train_ys)\n",
        "    acc = eval_model(model, featurizer, test_xs, test_ys)\n",
        "    print(acc, '\\n')\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Wxx5_aSR67h9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word features, 3000 examples\n",
            "0.812 \n",
            "\n",
            "lsa features, 3000 examples\n",
            "0.818 \n",
            "\n",
            "combo features, 3000 examples\n",
            "0.824 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# this will run a training experiment with all 3k examples in training set\n",
        "n_train = 3000\n",
        "training_experiment(\"word\", word_featurizer, n_train)\n",
        "training_experiment(\"lsa\", lsa_featurizer, n_train)\n",
        "training_experiment(\"combo\", combo_featurizer, n_train)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxfunCYh5nmZ"
      },
      "source": [
        "# Word2vec: word representations via neural model\n",
        "\n",
        "In this section, we'll train a word embedding model with a word2vec-style objective rather than a matrix factorization objective. This requires a little more work; we've provided scaffolding for a PyTorch model implementation below.\n",
        "If you don't have much PyTorch experience, there are some tutorials [here](https://pytorch.org/tutorials/) which may be useful. You may also find the classes `nn.Embedding` and `nn.EmbeddingBag` useful.\n",
        "\n",
        "Note: We will be implementing a CBOW model; that is, given a word's context, we will predict the central word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.5551,  0.7918, -1.3144],\n",
            "        [-0.2869,  0.6934,  0.6996],\n",
            "        [ 0.6405, -0.6530,  0.3147],\n",
            "        [ 0.5273,  0.6230,  1.1675],\n",
            "        [ 0.3608, -0.7298, -0.1664],\n",
            "        [-0.4091, -1.2875,  0.1101],\n",
            "        [ 2.4844, -1.3714, -0.0836],\n",
            "        [-0.7597, -0.3114,  0.6090],\n",
            "        [-0.9608,  0.7733,  1.1426],\n",
            "        [ 0.0307,  0.0443,  0.8211]], requires_grad=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.3400, 0.2196, 0.4404],\n",
              "        [0.3400, 0.2196, 0.4404]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "embedding = torch.nn.EmbeddingBag(10, 3, mode=\"mean\")\n",
        "print(embedding.weight)\n",
        "\n",
        "vals = torch.tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]], dtype=torch.long)\n",
        "v = embedding(vals)\n",
        "\n",
        "soft = torch.nn.Softmax(dim=1)\n",
        "soft(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "M1napibQ6aub"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "\n",
        "class Word2VecModel(nn.Module):\n",
        "    # A torch module implementing a word2vec predictor. The `forward` function\n",
        "    # should take a batch of context word ids as input and predict the word\n",
        "    # in the middle of the context as output, as in the CBOW model from lecture.\n",
        "    # Hint: look at how padding is handled in lab_util.get_ngrams when\n",
        "    # initializing `ctx`: vocab_size is used as the padding token for contexts\n",
        "    # near the beginning and end of sequences. If you use an embedding module\n",
        "    # in your Word2Vec implementation, make sure to account for this extra\n",
        "    # padding token in the input dimension and include the `padding_idx` kwarg.\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, padding_idx=2006):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.EmbeddingBag(\n",
        "            num_embeddings=embedding_size, embedding_dim=vocab_size)\n",
        "        self.soft = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Context is an `n_batch x n_context` matrix of integer word ids.\n",
        "        # In this case, n_context = 2 * window_size where window_size is defined\n",
        "        # in lab_util.py. This is because each word has both left and right context.\n",
        "        # This function should return an `n_batch x vocab_size` matrix with\n",
        "        # element i, j being the (possibly log) probability of the middle word\n",
        "        # in context i being word j.\n",
        "\n",
        "        # Your code here!\n",
        "        x = self.embedding(context)\n",
        "        x = self.soft(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOE4RyP9LX26"
      },
      "source": [
        "Train the model using the function below. Note that we use an [Adam optimizer](https://arxiv.org/abs/1412.6980). This is a fancy version of SGD which uses momentum and adaptive updates. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "ePgZlityuWr3"
      },
      "outputs": [],
      "source": [
        "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n",
        "    # This method takes in a corpus of training sentences. It returns a matrix of\n",
        "    # word embeddings with the same structure as used in the previous section of \n",
        "    # the assignment. (You can extract this matrix from the parameters of the \n",
        "    # Word2VecModel.)\n",
        "\n",
        "    tokenizer = lab_util.Tokenizer()\n",
        "    tokenizer.fit(corpus)\n",
        "    tokenized_corpus = tokenizer.tokenize(corpus)\n",
        "\n",
        "    ngrams = lab_util.get_ngrams(tokenized_corpus, window_size, pad_idx=2006)\n",
        "\n",
        "    device = torch.device('cuda')  # run on colab gpu\n",
        "    model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
        "\n",
        "    # What loss function should we use for Word2Vec?\n",
        "    loss_fn = nn.NLLLoss()  # Your code here!\n",
        "\n",
        "    losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        epoch_loss = 0\n",
        "        for context, label in loader:\n",
        "            # As described above, `context` is a batch of context word ids, and\n",
        "            # `label` is a batch of predicted word labels.\n",
        "\n",
        "            # Here, perform a forward pass to compute predictions for the model.\n",
        "            # Your code here!\n",
        "            preds = model(context)\n",
        "\n",
        "\n",
        "            # Now finish the backward pass and gradient update.\n",
        "            # Remember, you need to compute the loss, zero the gradients\n",
        "            # of the model parameters, perform the backward pass, and\n",
        "            # update the model parameters.\n",
        "            # Your code here!\n",
        "            loss = loss_fn(preds, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "    # Hint: you want to return a `vocab_size x embedding_size` numpy array\n",
        "    embedding_matrix = model.weight  # Your code here!\n",
        "\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "aaUy1cNuB3W1"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [162], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Feel free to change the hyperparameters!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Use the function you just wrote to learn Word2Vec embeddings:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m reps_word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_reps_word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_reviews\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn [161], line 14\u001b[0m, in \u001b[0;36mlearn_reps_word2vec\u001b[0;34m(corpus, window_size, rep_size, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m ngrams \u001b[38;5;241m=\u001b[39m lab_util\u001b[38;5;241m.\u001b[39mget_ngrams(tokenized_corpus, window_size, pad_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2006\u001b[39m)\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# run on colab gpu\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWord2VecModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     17\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch_data\u001b[38;5;241m.\u001b[39mDataLoader(ngrams, batch_size\u001b[38;5;241m=\u001b[39mn_batch, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/.virtualenvs/pset-env/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/.virtualenvs/pset-env/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.virtualenvs/pset-env/lib/python3.10/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/.virtualenvs/pset-env/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "File \u001b[0;32m~/.virtualenvs/pset-env/lib/python3.10/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "# Feel free to change the hyperparameters!\n",
        "# Use the function you just wrote to learn Word2Vec embeddings:\n",
        "reps_word2vec = learn_reps_word2vec(train_reviews, 2, 500, 10, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3oE-tpR7I39"
      },
      "source": [
        "After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMW4QND56bHF"
      },
      "outputs": [],
      "source": [
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-9CPSc7fi9"
      },
      "source": [
        "We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Yf6NMCXVx4"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
        "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
        "np.random.shuffle(zipped)\n",
        "zipped = zipped[:100]\n",
        "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
        "for token, cluster_idx in zipped:\n",
        "  word = vectorizer.tokenizer.token_to_word[token]\n",
        "  print(f\"{word}: {cluster_idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1TkENU78Wn"
      },
      "source": [
        "Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding. Implement the transform function in Word2VecFeaturizer to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5vjmRV6Dgbu"
      },
      "outputs": [],
      "source": [
        "def w2v_featurizer(xs):\n",
        "    # This function takes in a matrix in which each row contains the word counts\n",
        "    # for the given review. It should return a matrix in which each row contains\n",
        "    # the average Word2Vec embedding of each review (hint: this will be very\n",
        "    # similar to `lsa_featurizer` from above, just using Word2Vec embeddings \n",
        "    # instead of LSA).\n",
        "\n",
        "    feats = None # Your code here!\n",
        "\n",
        "    # normalize\n",
        "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "training_experiment(\"word2vec\", w2v_featurizer, 3000)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1nze9Jnmgw"
      },
      "source": [
        "# Experiments for HW1\n",
        "\n",
        "Below, you can implement experiments to answer the experimental questions in the HW1 handout. Please label each code cell with its relevant question part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9u7BNHuoB9O"
      },
      "outputs": [],
      "source": [
        "# Your code here!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('pset-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e2b7c7d14c2580883456174bb0caf07fa0db1f805ee5dff26967ad348472be82"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
